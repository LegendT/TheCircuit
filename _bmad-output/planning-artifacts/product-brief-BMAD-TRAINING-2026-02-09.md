---
stepsCompleted: [1, 2, 3, 4, 5]
inputDocuments:
  - PROJECT_BRIEF_V1.txt
  - bmad-code-org-bmad-method-8a5edab282632443.txt
  - VIDEO_SUMMARY.txt
  - BMAD V6 GitHub repo intelligence (fetched)
  - bmad_v_6_training_competitive_positioning_matrix.md
  - bmad_v_6_training_where_competing_offerings_fall_short.md
  - bmad_v_6_training_free_bmad_content_landscape.md
  - bmad_v_6_training_learner_journey_week_1_month_1 V2.md
  - bmad_v_6_training_minimum_valuable_programme V2.md
  - bmad_v_6_training_why_learners_say_this_is_exactly_what_i_needed V2.md
  - bmad_v_6_training_overstated_and_missing_elements V2.md
  - bmad_v_6_training_measuring_learner_impact_without_distortion.md
  - bmad_v_6_training_business_success_criteria_and_viability_milestones V2.md
  - bmad_v_6_training_leading_indicators_week_one_signal V2.md
  - bmad_v_6_training_metrics_we_deliberately_refuse_to_track V2.md
  - bmad_v_6_training_absolute_minimum_viable_programme_revalidated_scope V2.md
  - bmad_v_6_training_day_1_features_that_directly_solve_the_core_problem V2.md
  - bmad_v_6_training_what_would_feel_incomplete_if_missing_tiered_minimums V2.md
  - bmad_v_6_training_mvp_hard_no_list_scope_boundaries V2.md
date: 2026-02-09
author: Tone
---

# Product Brief: BMAD-TRAINING

## Executive Summary

This product is a **constrained professional decision environment** — the entry point into the BMAD ecosystem. It is not a course, not a training programme, and not a tutorial. It is a tool that creates the conditions for calibrated decision-making under consequence, then lets the user classify what happened.

The product resolves a calibration deficit: the gap between having access to powerful AI-assisted tools and knowing when to stop, what is enough, and when a decision is yours. It does this through constrained action, not through content.

The product operates as a **two-product strategy**:

1. **The Decision Circuit (MVP / Entry Product)** — A standalone constrained decision environment using a token-based access model. Professionals bring real, consequential decisions. The circuit forces clarity through a 5-move judgement sequence (eligibility, declaration, sufficiency, artefact, commitment) followed by behavioural classification. No BMAD knowledge required. Stands alone as a professional decision tool.

2. **BMAD Training (Future Ecosystem)** — For users who've experienced calibration through the circuit and want to apply it across the full BMAD development lifecycle — agent configuration, scope decisions, workflow transitions, sufficiency calls. This is where BMAD is explicitly taught, but only to people who already have the calibration skill.

The product targets professionals whose work is visible, whose judgement is reviewable, and whose decisions have cost — from agency leads and senior engineers to indie builders and accountability holders.

Security, accessibility (WCAG 2.2), and payment integrity are non-negotiable foundations, not afterthoughts.

---

## Core Vision

### Problem Statement

People across industries are adopting AI-assisted workflows — often enthusiastically, sometimes productively — but without a structured way to decide how much process is appropriate. The result is a calibration deficit: work gets produced, but decisions cannot be explained, defended, or repeated. Current AI training focuses on tools and prompts. Nobody is teaching when to stop.

### Problem Impact

- Teams and individuals oscillate between over-engineering (applying full methodologies to simple tasks) and under-engineering (shipping without defensible decisions)
- AI is experienced as a productivity accelerant but quietly erodes professional judgement — users outsource thinking without noticing
- Failure is silent: people drift away from structured methods, return to unstructured prompting, or try another framework — with no feedback loop
- Organisations cannot verify whether their teams are using AI responsibly or just quickly

### Why Existing Solutions Fall Short

- **Pragmatic Institute**: method-driven but not AI-native; static content model ages quickly; teaches best practices, not sufficiency decisions
- **Reforge**: excellent pedagogy but no canonical method; synthesis burden falls on the learner; completion does not imply competence
- **AIPMM**: outdated mental model; certification signals attendance, not applied competence; fundamentally pre-AI
- **Coursera/Udemy**: no coherence, no professional trust signal, severe content rot, no execution linkage
- **Free BMAD content (repo, docs, YouTube)**: BMAD is available but not taught; knowledge is descriptive, not instructional; no assessment, verification, or certification; no versioned learning path

The market teaches *what to do*. Nobody teaches *how to decide what to do — and when to stop*.

### Proposed Solution

A **constrained decision environment** that trains calibration through action, not content:

1. **Creates calibration, doesn't teach it** — the system never explains what calibration is; it forces the user into a constrained decision sequence where calibration is the only way through
2. **Works on real decisions from first use** — no hypothetical exercises; the user brings a real, consequential decision they currently face
3. **Forces judgement into visibility** — through structured constraint: ownership declarations, sufficiency boundaries, accepted risks, and defensibility tests
4. **Gates value on commitment, not time** — a token-based access model where eligibility failures are free and the circuit consumes the token only when a decision enters the sequence
5. **Acknowledges the real cost** — decisions under constraint feel exposed; the product names this honestly rather than pretending the experience is comfortable
6. **Provides dignified outcomes at every exit** — eligibility termination is successful calibration ("that decision didn't need structure"), not rejection; "not proceed" at commitment is valid, not failure

The product is fully asynchronous, accessible (WCAG 2.2), secure in payment handling, and designed as the entry point into the broader BMAD methodology ecosystem.

### Key Differentiators

1. **Action, not content** — the product contains no lessons, no theory, no curriculum; calibration is trained through constrained decision-making on real stakes
2. **Calibration over instruction** — resolves a calibration deficit, not a skills deficit; the user learns when to stop, not what to do
3. **Constraint is the product** — the value is in what the system refuses to let you do (hedge, over-research, outsource authority), not what it provides
4. **Token-based commitment model** — eligibility failures are free; the user pays only for decisions that enter the circuit; gaming is self-punishing through economics, not detection
5. **Gateway to BMAD ecosystem** — the circuit stands alone as a professional decision tool, but naturally surfaces the calibration skill that underpins the entire BMAD methodology
6. **First mover in a new category** — constrained professional decision environments don't exist as a product category; this defines it

### Pre-mortem Risk Analysis

The following failure scenarios were identified through pre-mortem analysis conducted during vision discovery. Each represents a plausible path to programme failure and includes specific prevention measures.

#### Risk 1: Founder Bottleneck on Content Updates (Likelihood: Medium / Impact: Medium)

*Substantially reduced by two-product strategy.* The circuit MVP contains no content — it is a constrained environment, not a curriculum. There is nothing to update when BMAD versions change because the circuit doesn't teach BMAD. This risk re-emerges at Tier B1 (BMAD Training) where methodology-specific content is introduced. At that stage, the original prevention applies: define what content must come from the founder vs what can be maintained by a system.

**Prevention (for future BMAD Training tiers):** Define before Tier B1 development what content must come from the founder vs what can be maintained by a system or contributor. The circuit MVP is immune to this risk.

#### Risk 2: Audience Split Dilutes Value (Likelihood: Medium / Impact: Medium)

*Substantially reduced by two-product strategy.* The circuit is a standalone decision tool — it doesn't target "anyone with a project." It targets anyone who makes consequential decisions under visibility. This is a broader but more coherent audience than "BMAD users." The audience split risk re-emerges at Tier B1 (BMAD Training) where methodology-specific content would need to serve different professional contexts.

**Prevention:** The circuit MVP markets to decision-makers under consequence, not to BMAD users specifically. Audience segmentation by role becomes relevant only when BMAD Training tiers are developed.

#### Risk 3: Free Content Narrows the Gap (Likelihood: Low / Impact: Medium)

*Substantially reduced by two-product strategy.* The circuit doesn't compete with free BMAD content — it's a different product category entirely. Free content teaches BMAD methodology. The circuit is a constrained decision environment. They're not substitutes. This risk re-emerges only if free decision tools appear that replicate the circuit's constraint-based approach.

**Prevention:** The circuit's value is in its constraints, not its content. There is no content to give away for free. The free eligibility attempts serve as the experiential boundary — the user experiences the product before paying for a full circuit.

#### Risk 4: Infrastructure Over-build Delays Launch (Likelihood: Medium / Impact: High)

"World-class security," "WCAG 2.2 compliance," and "enterprise-grade payment handling" consume 70% of the budget before a single lesson is written. Launch delays by months. Momentum dies. The market moves.

**Prevention:** Define a "secure enough to launch" baseline vs an "enterprise-grade target." Launch with Stripe (PCI handled externally), robust auth, and WCAG AA on core flows. Harden progressively as revenue justifies it. The V1 brief's own kill criterion — "overengineering security theatre" — applies to the build itself.

#### Risk 5: Certification Has No Market Pull (Likelihood: Medium / Impact: Low)

*Deferred by two-product strategy.* Certification is a Tier B2 feature — organisational BMAD adoption. It does not exist in the circuit MVP and won't be built until organic demand is proven. The original prevention is correct: delay certification until market pull exists. The two-product strategy enforces this naturally through proof gates.

**Prevention:** Certification is proof-gated at Tier B2. It cannot be built until organisational adoption is occurring organically. This risk is managed by architecture, not discipline.

#### Risk 6: Emotional Cost Drives Early Abandonment (Likelihood: Medium / Impact: Medium)

The circuit forces decisions under constraint. Users feel exposed — that's the design. But some may abandon mid-circuit (consuming their token) because the discomfort feels excessive rather than productive. The "sparse, serious, slightly exposed" principle is correct, but there's a line between productive discomfort and hostile UX.

**Prevention:** The token model provides natural protection — users who abandon mid-circuit have spent their token, creating a self-selecting population. Users who find the circuit too uncomfortable will not purchase additional tokens, which is the correct outcome. The key design question: ensure the discomfort comes from the decision (productive) not from the interface (hostile). UX must be clean, clear, and respectful even when the constraints are strict.

#### Risk Summary

| Risk | Likelihood | Impact | Status |
|------|-----------|--------|--------|
| Founder bottleneck on updates | Medium | Medium | Deferred — circuit has no content to update |
| Audience split dilutes value | Medium | Medium | Reduced — circuit targets decision-makers, not BMAD users |
| Free content narrows the gap | Low | Medium | Reduced — circuit doesn't compete with free content |
| Infrastructure over-build delays launch | Medium | High | Unchanged — still applies to circuit build |
| Certification has no market pull | Medium | Low | Deferred — proof-gated at Tier B2 |
| Emotional cost drives early abandonment | Medium | Medium | Managed — token model self-selects |

### Commercial Viability Challenges

The following challenges were surfaced through Shark Tank stress-testing of the value proposition during vision discovery. Each represents a hard question from a sceptical investor perspective that must be answered before the programme can be considered commercially viable.

#### Economics & Pricing

**Token pricing model is undefined (Severity: High):** The token model resolves the eligibility commercial risk but introduces a new question: what does a token cost? Single token, token packs, subscription — all viable, none validated. Market research is required. The pricing must reflect the value of a professional decision artefact, not the duration of an experience.

**Content update economics (Severity: Low):** *Substantially reduced by two-product strategy.* The circuit MVP has no content to update — it is a constrained environment. This challenge re-emerges at Tier B1 (BMAD Training) where methodology content is introduced. At MVP, the only update concern is constraint logic, not curriculum.

**Async completion rate tension (Severity: Low):** *Reframed by circuit model.* The circuit is a single-session decision environment, not an asynchronous course. There is no multi-week completion to track. A user either completes a circuit in one session or abandons it. The token model makes abandonment self-punishing. Traditional completion rate metrics don't apply.

#### Product & Delivery

**Delivery mechanism is clearer but UX is undefined (Severity: High):** The product is now a constrained decision environment — not a course platform. The user logs in, enters a decision, passes through 5 moves, receives an artefact. The conceptual model is clear. But the specific UX — web app, progressive form, conversational interface — is undefined. PRD must specify the delivery mechanism.

**Real-decision variability at scale (Severity: Medium):** *Reframed by circuit model.* Users bring their own real decisions. The circuit doesn't assess the quality of the decision — it forces structure, ownership, and commitment. There is no "correct answer" to evaluate. The constraint logic must work regardless of decision domain (technical, commercial, strategic, creative). This is a UX and constraint-design challenge, not an assessment challenge.

**Forced AI disagreement is no longer in scope (Severity: N/A):** *Removed.* The circuit MVP does not include AI disagreement exercises. This was a feature of the original training programme concept. It may re-emerge in BMAD Training tiers if appropriate.

#### Market & Audience

**Addressable market is broader but unvalidated (Severity: High):** *Reframed by two-product strategy.* The circuit targets anyone who makes consequential decisions under visibility — a far larger market than BMAD community members. But this market doesn't know it needs a "constrained decision environment." The product category doesn't exist yet. Market creation is harder than market capture. The product must be viable at small scale while the category is established.

**Non-technical audience is now the primary audience (Severity: Medium):** *Reframed.* The circuit doesn't require technical knowledge — it processes any consequential decision. The non-technical audience question flips from "can they use BMAD?" to "will they find a constrained decision tool?" Discovery and positioning for non-technical professionals is the challenge, not product fit.

**Dependency on BMAD's own release cycle (Severity: Low):** *Substantially reduced.* The circuit MVP has no dependency on BMAD versions — it doesn't teach BMAD. Constraint logic is independent of BMAD's methodology evolution. This dependency re-emerges at Tier B1 (BMAD Training) where methodology-specific content tracks BMAD versions.

#### Brand & Positioning

**Selling calibration requires buyer comprehension first (Severity: High):** *Still applies, potentially intensified.* "Calibration deficit" is intellectually precise but commercially opaque. As a standalone decision tool (not BMAD training), the product can't rely on BMAD community awareness. The purchase trigger must be viscerally concrete: "Bring a real decision. In 20 minutes you'll have a defensible artefact and you'll know which behaviour you over-use." The free eligibility attempts may serve as the experiential trigger — but this needs validation.

**Positioning must sell the artefact and the constraint (Severity: High):** *Reframed.* The positioning challenge shifts from "what's BMAD training?" to "why would I pay for a constrained decision environment?" The product must sell what the user gets (a defensible decision record) and what the experience does (forces clarity by limiting what you can do). Concrete claim: "People learn to recognise which behaviour they're over-using." This is observable, not abstract.

**Product name is a strategic decision (Severity: High):** *Elevated in importance.* The circuit is no longer "BMAD v6 Training Programme" — it's a standalone product that may not carry the BMAD name at all. The naming decision affects the entire two-product strategy: does the circuit carry BMAD branding (tying it to the ecosystem) or stand alone (maximising market breadth)? This must be resolved before launch.

#### Challenge Summary

| Challenge | Category | Severity | Status |
|-----------|----------|----------|--------|
| Token pricing model undefined | Economics | High | New — requires market research |
| Delivery UX undefined | Product | High | Reduced from Critical — concept clear, UX not |
| Addressable market broader but unvalidated | Market | High | Reframed — market creation challenge |
| Selling calibration requires buyer comprehension | Brand | High | Unchanged — potentially intensified |
| Positioning must sell artefact and constraint | Brand | High | Reframed — new positioning challenge |
| Product name is a strategic decision | Brand | High | Elevated — two-product naming |
| Real-decision variability at scale | Product | Medium | Reduced — constraint design, not assessment |
| Non-technical audience is primary audience | Market | Medium | Reframed — discovery challenge, not fit |
| Content update economics | Economics | Low | Deferred — circuit has no content |
| Async completion rate tension | Economics | Low | Reframed — single-session model |
| BMAD release cycle dependency | Market | Low | Reduced — circuit independent of BMAD versions |
| Forced AI disagreement | Product | N/A | Removed — not in circuit MVP |

## Target Users

### Primary Users

Primary users are defined not by job title or industry, but by a common condition: their work is visible, their judgement is reviewable, and failure has cost. BMAD is purchased as defensibility under scrutiny, not as learning.

#### Tier 1 — Acute, Consequential Pain (Fastest Conversion)

**The Agency Lead Who Lost Narrative Control**

- Agency founders, delivery leads, heads of product/engineering in services firms
- **Trigger:** AI-accelerated delivery meets client review, change request, or audit. They cannot clearly defend decisions — only that the work was fast
- **Core pain:** Credibility failure, not speed failure. "I can't explain why this is right — only that it's fast"
- **Why they pay quickly:** Revenue and retention are at risk. They need a visible delivery discipline that produces artefacts safe for external scrutiny
- **This is the strongest early buyer**, but only post-trigger

**The Senior IC Under Increased Visibility**

- Senior/staff/principal engineers, senior PMs and designers
- **Trigger:** AI increases output and visibility. Reasoning is questioned more often. Effort is no longer a shield
- **Core pain:** Loss of plausible deniability. "I'm now judged on judgement, not effort"
- **Why they pay:** Identity-level threat. Desire to operate in a visibly mature way
- **Note:** This group converts best when self-directed, not mandated

#### Tier 2 — Oscillating Pain (Converts After Fatigue or Failure)

**The Indie Builder with Structure Whiplash**

- Indie developers, solo founders, advanced builders
- **Trigger:** Framework adoption → overwhelm → retreat into unstructured prompting → repeated restarts
- **Core pain:** Oscillation, not ignorance. "I don't know which parts of structure actually matter"
- **Why they pay:** Exhaustion from context switching. Desire for sufficiency, not completeness
- **Conversion improves after visible stall or burnout**, not curiosity

#### Tier 3 — Latent Pain (Converts via Authority or Standardisation)

**The Accountability Holder in a Noisy System**

- Engineering managers, product leads, architects
- **Trigger:** Inconsistent AI usage across teams, quality disputes, blame landing upward
- **Core pain:** "If this goes wrong, I own it — but I don't control the process"
- **Why they pay:** Risk containment. Desire for neutral standards that shift debate from personality to process
- **Often funded via teams or organisations, not individuals**

#### Who Does Not Convert (Explicitly)

The programme is not for:
- **AI Tourists** — exploration without stakes, no external accountability
- **Throughput-Only Managers** — value speed over justification, see explanation as drag
- **Framework Collectors** — use structure to avoid judgement, optimise for coverage not clarity

These groups actively resist BMAD because it removes their protective layers. The programme must repel them clearly during onboarding, not attempt to convert them.

#### Common Denominator

Across all paying segments: work is visible, judgement is reviewable, failure has cost. The willingness-to-pay threshold is:

> "Someone else will judge my decisions. I need to be able to explain them."

### Segmentation Model

Segmentation is by measurable axis, not persona label. All learners share one entry point and one calibration experience (the Minimum Valuable Programme). Post-MVP, journeys branch along four axes:

1. **Exposure to external judgement** (low → high)
2. **Budget control** (personal → organisational)
3. **Time-to-value tolerance** (short → long)
4. **Psychological containment needs** (low → high)

| Segment | Profile | Primary Optimisation |
|---------|---------|---------------------|
| A: High Exposure / Org Spend | Agency leads, delivery heads, senior ICs with external scrutiny | Defensibility now — accelerated onboarding, decision records, reviewable artefacts |
| B: Low Exposure / Personal Spend | Solo creatives, indie builders | Cognitive relief — slower pacing, harder constraints, strong permission to stop |
| C: Medium Exposure / Limited Authority | Accountability holders inside organisations | Risk translation — artefacts for sceptical third parties, language for explaining restraint |

Premature segmentation increases dropout and weakens the shared mental model. Segmentation happens after the learner has experienced calibration once, not at sign-up.

### Buyer vs User

The user is the person who makes decisions under scrutiny, is exposed when work fails, and bears reputational or professional cost. The buyer is the person or entity who controls spend and wants reduced risk. These are often different people.

**Design principles:**
- The programme is designed for the user. Buyer needs are met through derivative artefacts, never through altered pedagogy
- If an element exists only to reassure the buyer, it does not belong inside the programme
- Marketing targets the exposed role, even when selling requires upward justification
- Translation scales with price, not with pedagogy

| Tier | Buyer-User Relationship | Buyer Surface |
|------|------------------------|---------------|
| MVP | User is buyer | Zero buyer surface |
| Agency | Buyer ≠ user | Light translation layer (decision summaries, before/after comparisons) |
| Organisation | Expanded translation | Still no behavioural distortion |

**Integrity test:** User asks "Did this sharpen my judgement?" Exposed role asks "Does this protect me under scrutiny?" Buyer asks "Can I justify paying without interfering?" If any answer requires changing the learning experience, the design is wrong.

### Secondary Users

At launch, the programme is fully self-service. Secondary human roles are excluded by design, not by oversight. This is a protective constraint: human presence introduces authority gradients, changes learner incentives, and shifts attention from decisions to approval.

**Launch phase:** No visible authorities. No public artefact sharing. No mentors. Minimal hygiene moderation only. Primary relationship: learner ↔ work ↔ consequences.

**Post-launch expansion** (sequential, one role at a time, each surviving a distortion audit):
1. Invisible hygiene moderation
2. Constraint-based peer artefact checks (artefact-focused, asynchronous, non-evaluative — no likes, rankings, or leaderboards)
3. Human verification for certification (verify sufficiency, not excellence — explicit criteria, no subjective scoring)
4. Optional, paid coaching (late-stage — questions only, never validates decisions or overrides learner judgement)

**Distortion audit:** Before introducing any secondary role, ask: "Does this increase the learner's reliance on external approval?" If yes, reject or redesign. If uncertain, delay.

The programme must explicitly name why mentorship is absent. Otherwise learners assume something is missing. Mentorship feels valuable because it reduces anxiety — and that reduction often comes at the cost of judgement development.

### User Journey

#### Discovery

Discovery channels act as filters, not amplifiers. The programme should be hard to stumble into and easy to recognise when needed. If onboarding has to persuade, discovery already failed.

**Primary channels (launch):**
1. Quiet practitioner-to-practitioner referral — highest quality, but trust is borrowed and expectations shaped by the referrer's framing
2. Long-form critical content (YouTube, talks, essays) — high idea affinity, moderate practical readiness

**Secondary:** Docs and repo adjacency with hard repulsion language

**Avoid:** Short-form social media, open community funnels, influencer-driven promotion, generic AI productivity framing

Discovery language should speak to imminent exposure, not only past mistakes. Best entrants arrive in a pre-defensive state — they sense fragility, anticipate scrutiny, have seen others fail.

> "If your AI-assisted work needs to stand up to questioning — this is for you."

#### Onboarding

Discovery paths converge within 30–60 minutes. After first calibration, origin is irrelevant. Onboarding does one thing only: convert psychological posture into action on a real decision.

**Channel-specific corrections:**
- Referral entrants → re-anchor around judgement, not speed
- Content entrants → collapse theory into one concrete decision quickly, interrupt analysis loops
- Tool-adjacent entrants → explicitly instruct to not follow everything, provide sanctioned shortcuts and omissions

#### Success Moment

The learner produces one small, defensible win on their own real work — a brief that is shorter and clearer than anything they usually produce, a decision they can defend calmly, or a justified stop that removes lingering anxiety. The artefact must be showable to a peer without embarrassment.

#### Long-term

The learner can say: "I trust my own decisions again — and I can explain them." Word-of-mouth is quiet but strong. Pricing remains firm. Learners return for deeper tiers.

### User Persona Focus Group Findings

The following findings were surfaced by having the four primary personas react to the programme design. These represent real concerns that must be addressed in the PRD and UX design.

#### Validated

- The core problem resonates across all four segments — "I can't defend my decisions" is universal
- The process sufficiency concept (deciding how much BMAD to apply) is the single most desired capability across all personas
- Real-project work is wanted by everyone, but the definition of "real project" varies dramatically

#### Challenged

- **Time availability:** Agency leads under fire may not have 30 min/day. The programme must support flexible time models (30 min/day or 2-hour weekend blocks)
- **Project type flexibility:** "Real project" must include tickets, feature specs, and existing codebases — not just greenfield work. A staff engineer's "real work" is Jira tickets, not a side project
- **Isolation at launch for solo builders:** Solo builders already work alone. If the paid programme is even more isolated than the free community experience, that may feel backwards. Async peer artefact reviews (non-evaluative) may be needed earlier than planned
- **Buyer visibility at team scale:** Team/org buyers need minimal measurable output — completion status plus artefact summaries — even at MVP. The "zero buyer surface" principle may need adjustment for team pricing
- **Certification timing gap:** Both agency leads and senior ICs need something to show externally now, not after market recognition develops. An interim signalling artefact is needed before formal certification
- **Bidirectional calibration:** The programme addresses AI over-trust but not AI under-trust. Some senior engineers over-engineer precisely because they distrust AI. The programme must calibrate in both directions — "stop trusting blindly" and "stop adding complexity when the AI is right"

#### Missing from Current Design

| Gap | Affected Personas | Priority |
|-----|------------------|----------|
| Flexible time models (daily or weekend blocks) | Agency leads, senior ICs | High |
| Project type flexibility (tickets, specs, existing codebases) | Senior ICs, indie builders | High |
| Bidirectional calibration (over-trust and under-trust) | Senior ICs | High |
| Minimal buyer visibility for team purchases | Accountability holders | High |
| Immediate external signalling artefact (pre-certification) | Agency leads, senior ICs | Medium |
| Earlier async peer connection for solo segments | Indie builders | Medium |

## Success Metrics

### Learner Success Metrics

Learner success is measured through **triangulation across three observable dimensions**, not through conventional training KPIs. Judgement cannot be measured directly — it can only be inferred from behaviour under mild constraint.

**1. Ownership (Primary Signal)**
- Learner uses first-person decision language ("I chose…", "I rejected X because…")
- Learner accepts consequence without deferring to AI, framework, or authority
- Negative indicators: appeals to "best practice", over-justification, framework attribution
- Artefact quality is necessary but insufficient — artefacts can improve cosmetically without judgement improving

**2. Stability Under Light Questioning**
- Learner responds to one mild challenge ("Why not the obvious alternative?") with short clarification and clear trade-off restatement
- Does not escalate, over-explain, or appeal to authority
- Defence speed matters only when paired with omission awareness — fast answers without conscious trade-offs are noise

**3. Sufficiency Boundaries (Hardest to Fake)**
- Learner names explicit stopping points and exclusions
- Articulates what would change the decision and what wouldn't
- Absence of boundaries = absence of calibration
- Reduced anxiety is inferred through decision finality, not absence of activity

**Minimal Viable Measurement Set:** Each learner submits (1) one real decision artefact, (2) one short defence answering "Why this, not the most obvious alternative?", and (3) one explicit omission or stop decision. Assessment is binary: calibration present / not yet present. No scoring. No ranking.

**The Hard Diagnostic Question:** "Before this, how would you have justified this decision? How do you justify it now?" We look for a change in reasoning posture, not correctness. No shift = design failure.

**The Meta-Signal:** When BMAD is working, learners stop asking "Am I doing this right?" and start asking "Is this sufficient for the risk I'm carrying?" That question is not taught — it emerges.

**Final Design Constraint:** Any measurement added must satisfy this test: "Does this make learners rely more on their own judgement — or on the metric itself?" If it's the latter, it does not belong.

### Business Objectives

Business success is defined as **commercial viability without method corruption**. The governing question is: "Is this working as a business without corrupting the method?"

**3-Month Horizon — Proof of Viability**

The 3-month question: "Would this survive without my evangelism?"

All five conditions required:
1. **Correct buyer fit** — majority of buyers have real decision exposure; purchases occur without heavy persuasion
2. **Reuse without prompting** — learners apply BMAD to a second decision outside the programme context
3. **Quiet pull into real work** — artefacts shared upward or sideways; BMAD language appears in work conversations
4. **Revenue covers reality** — platform + maintenance costs covered; founder time compensated at a defensive rate (magnitude secondary, sufficiency primary)
5. **Sustainable attention load** — fewer than ~10-15% require mechanical support; questions are about judgement, not usage

**3-Month Failure Signals (Explicit Kill/Pause Criteria):**
- Buyers are curious but not reusing
- Learners ask for more content instead of clearer decisions
- Revenue depends on discounts or urgency tactics
- Support demand trends upward

These are structural misalignments, not marketing issues.

**12-Month Horizon — Proof of Sustainability**

The 12-month question: "Is this now a practice people rely on, not a programme they completed?"

Sustainability conditions:
1. **Clear product ladder (earned)** — advanced tiers exist because demand pulled them, not because the roadmap promised them
2. **Organisational adoption signals** — team or agency purchases; internal reuse without re-training
3. **Repeat revenue with low friction** — renewals or follow-ons without re-selling; 30-50% of early buyers return in some form
4. **Defensible revenue band** — sufficient to maintain content, infrastructure, and add selective human roles
5. **Method integrity under load** — no increase in performative metrics; calibration outcomes stable; buyer pressure resisted successfully

**The Minimum Viable Business Test:** "People with real decision risk pay, reuse without prompting, and bring BMAD into their work — without us having to chase them." If any clause fails, the business is not yet viable.

**Time Cost as First-Class Metric:** The business is failing if it requires constant explanation, high-touch reassurance, or manual correction of misuse — even with revenue.

**Drift Detection:** Early integrity warnings include pressure to add more metrics, requests for leaderboards or progress tracking, and buyer requests to modify pedagogy for reporting. These are not feature requests; they are integrity warnings.

**Optimisation Priority Order (Non-Negotiable):**
1. Buyer exposure fit
2. Reuse under consequence
3. Quiet organisational pull
4. Revenue sufficiency
5. Only then, scale

### Key Performance Indicators

**Leading Indicators (Week-One Signal Clusters)**

Instead of a single KPI, use signal clusters across three dimensions:

| Dimension | Positive Signals | Negative Signals |
|-----------|-----------------|-----------------|
| **Intent** (earliest) | "I'm going to use this on X next"; saving/reopening framework unprompted | "This is interesting, but…"; requests for more explanation without application |
| **Ownership** (most diagnostic) | First-person decision language; explicit rejection of alternatives | Appeals to authority; over-justification |
| **Boundary clarity** (hardest to fake) | Named stopping points; clear sufficiency criteria | Endless extension; reluctance to commit |

**Week-One Diagnostic Checklist:**
1. Did any learner show intent to reuse BMAD on another decision?
2. Did any learner explicitly own a decision without deferring to authority?
3. Did any learner articulate a justified stopping point?

Interpretation: 0/3 → pause and investigate onboarding. 1/3 → continue cautiously. 2/3+ → strong signal the programme has legs.

**Signals to Actively Ignore:** High early enthusiasm, praise for content clarity, requests for more material, high completion velocity. These correlate poorly with calibration. Some early enthusiasm actively predicts later failure (requests for more templates, desire to formalise everything, praise focused on structure not decisions).

**False Positive Warning:** Unprompted second use is the strongest signal when it appears, but its absence is not failure unless paired with other negative indicators. Some high-value users have only one live decision in the first week. Language shift should be read as structural clarity, not keyword usage — some learners internalise calibration before adopting the vocabulary. Early stopping is positive only when paired with explicit justification.

**Metrics We Deliberately Refuse to Track**

The programme operates under a standing metric-governance policy. A metric must be refused if any of the following are true: learners could optimise for it without improving judgement; it creates social comparison or ranking; it shifts authority away from the learner; it incentivises continuation over sufficiency. If unsure, default to refusal.

| Refused Metric | Category | Position |
|---------------|----------|----------|
| Peer ratings / upvotes | Authority-substituting | Never introduce |
| Instructor scores | Authority-substituting | Never introduce |
| Leaderboards | Behaviour-distorting | Never introduce |
| Gamified progress | Behaviour-distorting | Never introduce |
| Speed benchmarks | Behaviour-distorting | Never introduce |
| Volume of artefacts | Behaviour-distorting | Never introduce |
| Learning hours | Buyer-comfort | Internal diagnostic only, never framed as success |
| Completion percentages | Buyer-comfort | Silent friction detection only, never reported upward |
| Engagement dashboards | Buyer-comfort | Used only to identify usability problems |
| Certification pass rate | Buyer-comfort | Binary sufficiency only, no grading or trend optimisation |

**Diagnostic vs Evaluative Distinction:** Diagnostic metrics (used privately to improve the system) are tolerated. Evaluative metrics (used publicly to judge success) must be resisted. Engagement can be monitored privately for usability issues, but never elevated to success criteria.

**Metric Admission Test:** Before admitting any metric, ask: (1) Would learners change behaviour if they knew this was measured? (2) Could someone improve this metric without improving judgement? (3) Would this ever be shown to a buyer or used for comparison? If yes to any → refuse.

**Final Integrity Question:** Every business decision should answer: "Does this increase the number of defensible decisions being made — or just the number of people touching the programme?" Only the first produces durable success.

### Self-Consistency Validation Findings

The following tensions were surfaced through self-consistency validation of the four metrics components (learner success, business objectives, leading indicators, refused metrics). These represent internal contradictions or gaps that must be resolved during PRD development.

#### Alignment Confirmed

- **North star coherence is strong** — all four components point at the same outcome: defensible decisions made under consequence
- **Ownership appears in all three measurement layers** — learner metrics (primary signal), leading indicators (most diagnostic), business objectives (reuse implies ownership)
- **The refused metrics genuinely protect the other three** — tracking completion rates would distort ownership signals; gamified progress would corrupt sufficiency boundaries

#### Tensions Requiring Resolution

**Tension 1: Reuse as business success vs. sufficiency as learner success (Severity: High)**

The business model requires "reuse without prompting" — learners apply BMAD to a second decision, then a third. But the learner model explicitly teaches sufficiency — knowing when to stop. A learner who completes the MVP, produces one genuinely calibrated artefact, and decides "this is sufficient for my needs" is a learner success but a business failure signal. The framework implicitly depends on learners wanting more, but the pedagogy explicitly teaches them to want enough.

*Question for PRD:* Is reuse a genuine business signal, or is it a dressed-up engagement metric that survived the refusal list because it sounds philosophical?

**Tension 2: No bridge between week-one signals and 3-month criteria (Severity: High)**

Leading indicators ask: "Did ANY learner show intent/ownership/boundary clarity in week one?" The 3-month criteria ask: "Do the MAJORITY of buyers have correct fit, and are learners reusing?" There is no defined bridge between these checkpoints — no week-four signal, no month-two diagnostic. If week one looks good but month three fails, eight weeks of diagnostic visibility are lost.

*Question for PRD:* What should you see at week four that tells you you're on track — without introducing the metrics you've refused to track?

**Tension 3: Revenue diagnostics are blind without refused metrics (Severity: Medium-High)**

"Revenue covers reality" is a 3-month condition. But if revenue fails, diagnosis requires understanding where people drop off — which is operationally a completion/engagement question. The "silent friction detection" carve-out exists but creates an uncomfortable position: tracking data privately while refusing to call it a metric.

*Question for PRD:* Does the framework need a clearer operational distinction between diagnostic telemetry (system health) and success metrics (programme integrity)?

**Tension 4: "Quiet pull into real work" is unobservable under current rules (Severity: Medium)**

The 3-month condition "artefacts shared upward or sideways; BMAD language appears in work conversations" cannot be verified without tracking usage outside the programme or relying on self-report — which reintroduces the bias the framework claims to avoid.

*Question for PRD:* Is this a measurable business condition or an aspirational indicator that sounds good but can't actually be checked?

**Tension 5: Binary assessment creates a certification gap (Severity: Medium)**

Assessment is binary: calibration present / not yet present. But focus group findings identified that learners need an interim signalling artefact before formal certification has market recognition. If there is no gradation, there is no obvious basis for an interim artefact.

*Question for PRD:* Does the binary model need a "calibration in progress" state, or would that compromise the integrity of the binary assessment?

| Tension | Severity | Status |
|---------|----------|--------|
| Reuse vs. sufficiency misalignment | High | Requires resolution in PRD |
| Week-one → 3-month diagnostic gap | High | Requires resolution in PRD |
| Revenue diagnosis under refusal rules | Medium-High | Requires clarification in PRD |
| "Quiet pull" observability | Medium | Requires resolution in PRD |
| Binary assessment vs. interim signalling | Medium | Requires resolution in PRD |

## MVP Scope

### Strategic Positioning: Entry Product / Ecosystem Model

A critical insight emerged during scope stress-testing: the constrained decision circuit is not BMAD training. It is a **standalone professional decision environment** that surfaces the calibration skill BMAD is built around. BMAD training is what comes next — for people who've experienced calibration and want to apply it across a full development lifecycle.

**The two-product strategy:**

| | The Circuit (MVP) | BMAD Training (Future) |
|---|---|---|
| **What it is** | A constrained decision environment | Methodology training for AI-native development |
| **What it teaches** | Calibration (implicitly, through action) | How to apply calibration across BMAD workflows, agents, and artefacts |
| **Who buys it** | Anyone who makes consequential decisions | People who've used the circuit and want the full methodology |
| **BMAD dependency** | None — stands alone | Full — requires understanding of BMAD V6 |
| **Relationship** | Entry product / gateway | Ecosystem / upsell |

This means:
- The circuit can be marketed on its own merits without needing to explain BMAD upfront
- The calibration capture label ("BMAD works with these decision constraints") is a **breadcrumb** to the ecosystem, not a pedagogical moment
- BMAD training becomes the next tier for people who want to apply calibrated decision-making across an entire development lifecycle
- The product name, brand positioning, and BMAD relationship should be resolved before BMAD training development begins

**Sections revised:** Executive Summary, Proposed Solution, Key Differentiators, Pre-mortem Risk Analysis, and Commercial Viability Challenges have been updated to reflect the entry product / ecosystem model. Problem Statement and Problem Impact remain unchanged — the calibration deficit thesis applies equally to both products.

### Core Features

The MVP is not minimal content. It is **maximal constraint around minimal moves**. The scope has not expanded through discovery — it has become harder to misuse. That hardness is the product.

The MVP is a constrained decision environment with a **calibration capture** — not an educational wrapper. The critical distinction: the system never explains calibration to the learner. It creates the conditions for calibration, then gives the learner a classification tool — not a map.

This resolves the core commercial tension: learners need post-hoc coherence to value an experience (without it, value decays and attribution drifts), but pre-action content kills the intervention. The solution is classification after experience, not instruction before it.

**The hard line:**

> We do not teach calibration. We create the conditions for it, then give learners a classifier — not a map.

---

#### The Judgement Circuit (5 Moves)

The circuit delivers a single, constrained judgement sequence that a learner completes on one real decision. These five moves are not a workflow — they are an irreducible sequence that restores decision ownership under consequence. The cold start is not a UX choice — it is the mechanism. No content precedes it.

**Move 0: Orientation (Single Sentence)**

Before any interaction, the learner sees:

> "You are not here to learn BMAD. You are here to decide."

Then the system immediately asks for a real decision. No content. No theory. No warm-up. No philosophy briefing. The shock of immediate action is the intervention's first move.

**Move 1: Eligibility (Kill Switch — No Reasons Given)**
- Learner must state why the decision deserves structure
- Learner must name at least one decision that does *not* qualify
- Failure to disqualify anything = misuse
- If eligibility fails, the programme ends — and the system confirms: "Stopping here is correct"
- **No explanation of why the decision failed.** No criteria revealed. No rubric externalised. The moment you say "this decision is not suitable because X" you have turned calibration into pattern-matching and created a retry-with-better-writing loop
- Eligibility termination is framed as successful stopping, not rejection

**Move 2: Declaration (Ownership Lock)**
- Learner declares the decision in first-person terms
- Must include explicit ownership of consequence
- Disallowed language is actively blocked: "The framework suggests…", "AI recommended…", "We decided…" (without role clarity)
- No examples shown — the learner must produce their own framing
- A declaration without ownership is a null step

**Move 3: Sufficiency (Central Move — The Core Learning Event)**
- Learner names what is enough
- Learner names what is excluded
- Learner names the accepted risk
- All fields are mandatory and short
- This is where stopping confidence is trained — if sufficiency is weak, no later step can compensate

**Move 4: Artefact Assembly (Two-Minute Defence Test)**
- Minimal artefact composed entirely of learner inputs — no AI summarisation that replaces thinking
- Contains: the decision, the sufficiency rationale, the rejected alternative
- One prompt: "Could you defend this in under two minutes?"
- Artefact must be persistently saved (session persistence is required even at Tier 1)

**Move 5: Stop/Go (Irreversibility Marker)**
- Learner must choose: commit or explicitly not proceed
- If commit: one downside is named, one reconsideration trigger is named
- No hedging language. No "next steps" padding
- The decision is marked complete with visual and linguistic finality
- Re-opening is possible but framed as reconsideration, not continuation

---

#### Calibration Capture v2 (Classification, Not Reflection)

The system never tells the learner what their experience means. But it must prevent value evaporation — the phenomenon where a learner feels relief and clarity during the circuit but cannot articulate it afterwards, leading to value decay ("I just thought harder"), attribution drift ("I already knew this"), and commercial death (no reuse, no word-of-mouth, no buyer translation).

The solution is not reflection. Reflection asks how it felt. Classification asks what changed. The distinction matters: reflection invites narrative and self-expression; classification forces the learner to map experience to observable behaviour. This is post-mortem classification, not introspection.

**The hard line:**

> We never ask how it felt. We only ask what changed. If a prompt can't be answered by pointing to a behaviour, boundary, or cost, it doesn't belong.

**Calibration Capture Design Principles**

These principles govern all post-decision classification prompts. They emerged from failure mode analysis and are non-negotiable at PRD stage.

1. **No "null" options.** Anything equivalent to "None of these," "I'm not sure," or "N/A" is a calibration sinkhole. If the prompt exists, it must force classification or be skippable entirely. Skipping is the escape hatch — not a safe-harbour option within the prompt.

2. **Language must describe behaviour, not character.** Options operate at the level of observable actions, situational constraints, and external forces — never internal qualities. "I was still gathering information" (behaviour) not "Information-seeking tendency" (character).

3. **Sophistication is distortion.** Any option that sounds like something a "good decision-maker" would say, echoes decision-theory language, or feels impressive will be over-selected. Plain language beats precise language.

4. **External pressure is not a failure.** A decision closing because a client deadline hit, a dependency resolved, or a window closed is real calibration under constraint — not lesser calibration. The system must not privilege internal virtue over external reality.

5. **Naming > Narrating.** Noun phrases over sentences. Labels over stories. Fragments over explanations. The moment learners can explain, justify, or contextualise, they start performing coherence instead of naming mechanics. Free-text fields capped at 50 characters.

---

**After Full Circuit Completion (Post Stop/Go)**

Appears only after the irreversibility marker and exit state. Optional. Skippable. No explanation before.

**Title displayed to learner:**

> What changed to allow this decision to close? *(Optional. Skip if you don't want to classify it.)*

**Prompt 1: Behavioural Interruption**

> Which behaviour stopped? *(Choose the closest fit.)*

- I was still gathering information
- I was still asking for input
- I was still running tools or prompts
- I was still changing what the decision was about
- The decision closed without resistance
- Something else: __________ *(50 chars)*

All options first-person, neutral, behaviour-level. No safe harbour — "something else" still forces naming. "The decision closed without resistance" legitimises already-calibrated cases as a genuine signal rather than a cop-out. No answer is "correct" — but not all answers are equivalent.

**Prompt 2: Sufficiency Boundary**

> What actually forced the decision to close? *(Not what should have.)*

- The cost of waiting longer
- An external deadline
- Accepting unresolved unknowns
- Limited reversibility
- Dependency on someone else
- Something else: __________ *(50 chars)*

"Actually" explicitly counters aspiration bias. Time pressure and cost of delay collapsed into "the cost of waiting longer." External deadline separated and legitimised as real calibration under constraint. "Accepting unresolved unknowns" is concrete where "unknowns accepted" was an epistemic flex. This prompt captures what applied pressure, not what sounds wise.

**Prompt 3: Cost Acceptance**

> Did this decision leave something unresolved that you'd normally try to fix?

- Yes
- No

If Yes, constrained label:

> Unresolved: __________ *(50 chars)*

Placeholder examples *(greyed, not selectable):* speed · certainty · alignment · completeness · precision

"Unresolved" removes moral weight. "Fix" is pragmatic, not accusatory. 50 characters prevents narration — enough for a label, not enough for a story. Single cost forces prioritisation, which is itself a calibration act.

**The Label (Optional — May Be Removed)**

After submission (or skip), one sentence:

> "BMAD works with these decision constraints."

No jargon. No reveal. No "you learned." Offensively flat. If the classification is sufficient on its own — and it may be — this label can be removed entirely. If the learner later encounters BMAD language elsewhere, recognition will happen then. The PRD should A/B test label-present vs label-absent.

---

**After Eligibility Exit (Stop)**

Optional classification. Skippable.

> Which property made this decision unsuitable for structure?

- The consequences were low
- The outcome depends on someone else's action
- The decision was already settled
- The cost of structuring exceeded the risk
- Something else: __________ *(50 chars)*

Options reframed as properties of the decision, not deficiencies of the learner. "The consequences were low" rather than "It had no real consequence." No "I'm not sure" — skipping is the escape hatch. No correction. No rubric revealed. No retry mechanism. The session ends. The learner has classified the decision's properties without being told what the right answer was.

**After "Not Proceed" at Stop/Go**

> You chose not to commit. Your decision record is saved.

Then one optional classification:

> What stopped you? *(Choose the closest fit.)*

- The risk was higher than I'd accept
- I don't have enough information yet
- The decision isn't actually mine
- I'm not confident in my sufficiency rationale
- Something else: __________ *(50 chars)*

Their deferred artefact is saved and accessible. No interpretation of their choice. No "this is a valid learning moment."

---

**What the Calibration Capture Provides**

*For the learner:*
- They know which lever actually closed the decision
- They can reuse that lever next time without thinking "BMAD"
- They feel competence, not inspiration
- They leave with a sharper internal classifier, not a better story

*For the product:*
- Calibration signals without scoring
- Preserved silence
- No course-ification
- Repeat-use legibility (the learner recognises the mechanics next time)

*For buyers:*
- A concrete claim: "People learn to recognise which behaviour they're over-using"
- That's observable. Not vibes

**Design Constraints for Calibration Capture:**
- Appears only after Stop/Go or eligibility exit (never before or during the circuit)
- All prompts are optional — the learner may skip every one
- The system never responds to classifications (no validation, no "interesting choice!")
- No open-ended reflection prompts — all forced-choice or constrained free-text (50 chars max)
- All forced-choice options use first-person, behaviour-level, neutral language
- No null options ("None of these," "I'm not sure," "N/A") — "Something else" with naming replaces all safe harbours
- The label is optional and subject to A/B testing at PRD stage
- Total additional time: under 90 seconds if engaged, 0 if skipped
- This is not a debrief. It is a post-mortem

---

#### The Exit State

After the calibration capture (or immediately after Stop/Go if all prompts are skipped), the experience ends. No next module. No upsell. No progress indicator.

The designed silence remains intact. The calibration capture does not replace silence — it creates a brief classification window before silence begins.

**Completeness Signals (All Five Required)**

Across all users, the MVP feels complete only if all five signals are present:
1. **Forced action** — a real decision is required
2. **System pushback** — constraints resist over-work
3. **Persistent artefact** — something remains afterwards
4. **Psychological finality** — the learner knows it is done
5. **Behavioural classification** — the learner has had the opportunity (not the obligation) to identify which mechanics closed their decision

If the first four are present but the fifth is absent, users describe the experience as "powerful but I can't explain why" — which kills word-of-mouth and buyer translation. Classification converts felt experience into nameable mechanics.

#### Access Model: Token-Based Repeatability

The MVP uses a **token model** that resolves the eligibility commercial risk and enables repeat use without introducing unlimited free access.

**How tokens work:**

| Event | Token Status |
|-------|-------------|
| Purchase | Learner receives token(s) — one-off or subscription (pricing requires research) |
| Enter decision → eligibility fails | Token **preserved** — try again with a different decision |
| Enter decision → eligibility passes → enters circuit | Token **consumed** |
| Complete circuit + artefact | Token spent, artefact delivered |
| Abandon mid-circuit | Token spent, partial record saved |
| Game eligibility with hollow decision | Token spent on worthless artefact — **self-punishing** |

**Key design decisions:**

**Eligibility failures are free.** The learner is not charged for failed eligibility. They keep their token and can bring a different decision. This completely resolves the commercial risk of post-purchase eligibility termination. A learner who fails eligibility three times before finding a suitable decision has practised decision discrimination three times — that's the product working, not failing.

**No cap on eligibility attempts.** A learner can attempt eligibility as many times as needed before consuming a token. Each attempt runs the post-eligibility classification prompt, which means repeated eligibility failures are themselves calibration practice. The risk that someone uses eligibility as free calibration training (without ever consuming a token) is accepted — if they're learning to discriminate decision types through repeated attempts, the product is delivering value even without a completed circuit.

**Entering the circuit consumes the token.** The moment eligibility passes and the learner enters Move 2 (Declaration), the token is spent. Whether they complete the circuit, produce a strong artefact, or abandon mid-way — the token is gone.

**Gaming is self-punishing.** The system does not need to detect gaming. If a learner forces a hollow decision through eligibility, the circuit exposes it: sufficiency will be weak, the artefact will be thin, the defence prompt will feel empty. They've spent their token on a worthless journey. The economics enforce honesty without fraud detection.

**Repeat use builds a decision portfolio.** Each completed circuit produces its own artefact. Over time, the learner accumulates a portfolio of decision records. This addresses the original value concern: the product is not a single-use exercise — it's a professional decision environment the learner returns to whenever they face a consequential decision.

**Pricing model (requires research):**
- One-off token purchase or subscription — both viable, research needed
- Token price point to be determined through market research
- Possible models: single token (premium), token packs (volume discount), subscription (N tokens per month)

**What repeatability does NOT introduce:**
- No decision history dashboard (artefacts are stored, not displayed as a timeline)
- No cross-decision analytics or progress tracking
- No gamification of decision count
- No comparison between decisions
- Each circuit is independent — no continuity between sessions beyond artefact storage

**Infrastructure Requirements (MVP Only)**
- Authentication and secure access
- Payment processing (Stripe — PCI handled externally) with token management
- Session persistence (saved artefacts survive logout)
- Token ledger (purchase, consumption, and preservation tracking)
- Artefact portfolio storage (multiple artefacts per user, individually accessible)
- WCAG AA compliance on core flows
- Constraint enforcement (language blocking, field limits, mandatory fields)
- Calibration capture rendering (three exit-path variants, all optional, forced-choice UI)
- Classification response storage (learner selections saved as internal diagnostics, never surfaced)
- No dashboards, no analytics visible to learners

### Out of Scope for MVP

All exclusions are **temporal, not permanent**. Every "no" exists to protect proof — proof that BMAD restores judgement before it reassures buyers, scales teams, or accumulates features. Each exclusion has a proof-based re-entry condition, not a roadmap promise.

**Unifying Doctrine:** Every MVP exclusion exists to protect one thing: the ability to observe unprompted, second-use of BMAD on real decisions. Anything that introduces reassurance, signalling, or coordination before that signal appears is excluded.

**Category A: Authority Substitutes**

| Excluded Feature | Reason | Re-entry Condition |
|-----------------|--------|-------------------|
| Certification | Introduces pass/fail optimisation; shifts focus from ownership to credential acquisition | Reuse under pressure is already happening; certification can verify sufficiency without scoring |
| Mentors, coaches, or live review | Create dependency; replace judgement with reassurance | Learners demonstrate stable ownership without support |

**Category B: Premature Coordination**

| Excluded Feature | Reason | Re-entry Condition |
|-----------------|--------|-------------------|
| Team pricing / seat management | Implies rollout before value is proven; pulls procurement logic into MVP | Quiet organisational pull — teams asking, not being sold |
| Collaboration or co-authoring | Dilutes ownership; obscures accountability | Read-only artefact sharing is already trusted and used |

**Category C: Reassurance & Optics**

| Excluded Feature | Reason | Re-entry Condition |
|-----------------|--------|-------------------|
| Dashboards, progress indicators, KPIs | Encourage performance over judgement; undermine finality | None at learner level; only allowed as internal diagnostics |
| Community, forums, peer feedback | Introduce comparison and signalling; shift authority outward | Only after judgement independence is demonstrably stable |

**Category D: Scope Dilution**

| Excluded Feature | Reason | Re-entry Condition |
|-----------------|--------|-------------------|
| Multiple audience tracks | Weakens constraint language; obscures core problem | Core intervention works reliably for one exposure profile |
| Extended theory or concept libraries | Delay action; encourage consumption over decision-making | Learners repeatedly request explanation after using the system successfully |
| Video instruction | Passive consumption; authority gradient from instructor presence | N/A — may never be appropriate for core experience |
| Non-technical audience paths | Decision exposure unvalidated for non-technical roles at MVP | Demand validated through organic enquiry, not marketing |

**Scope Integrity Test:** Before adding anything, ask: "Would this make it harder to tell whether BMAD restored judgement — or just easier for someone to feel comfortable?" If it increases comfort before proof, it is out of scope.

**The MVP Must Feel:** Sparse. Serious. Slightly exposed. If the product feels safe, familiar, or impressive too early, it has failed.

### MVP Success Criteria

MVP success is evaluated through the metrics framework defined in Step 4, applied specifically to the constrained scope above.

**The Irreversibility Test (Core Product Gate):**

> "After doing this once, could the learner ever fully return to their previous decision behaviour?"

If the answer is yes, the MVP is too soft. If the answer is no — even for a minority of learners — the core intervention works.

**Day-1 Product Test:**

If a learner completes Day 1 and says: "I didn't realise how much I was over-working decisions until this forced me to stop" — the product works. If they ask: "What should I do next?" — the design has failed.

**Week-One Gate (from Step 4):**
Apply the week-one diagnostic checklist. 0/3 signals → pause and investigate onboarding. 1/3 → continue cautiously. 2/3+ → strong signal the MVP has legs.

**3-Month Go/No-Go Decision:**
Apply the 3-month viability conditions from Step 4. All five must hold. If any kill/pause criteria trigger (buyers curious but not reusing, learners asking for more content not clearer decisions, revenue dependent on discounts, support trending upward), pause or redesign before proceeding to Tier 2.

**The PDF Test:**

> "Could this be replaced by a static document without losing behavioural force or emotional finality?"

If yes, something essential is missing — usually constraint, persistence, or irreversibility.

### Future Vision

Future expansion follows a **proof-before-promise** model across two product lines: the decision circuit (the entry product) and BMAD training (the methodology ecosystem). Each tier is earned by demand, not promised by roadmap. No tier is built until the previous tier's proof conditions are met.

**Product Line 1: The Decision Circuit (Entry Product)**

**Tier 1 — MVP (Current Scope)**

The constrained decision environment with token-based access, calibration capture, and artefact portfolio. Stands alone. No BMAD dependency.

**Tier 2 — Decision History & Continuity**

Adds memory and protection against hindsight rewriting across multiple decisions. Core additions: decision history with comparative continuity (pattern recognition across decisions over time), clear distinction between completion and reconsideration, reminder of original sufficiency boundary when revisiting.

*Proof required before building:* Tier 1 users are consuming multiple tokens and returning unprompted. Reuse is organic, not incentivised.

**Tier 3 — Shareable Decisions**

Adds decisions that can be shared, read, and trusted by others without explanation or participation. Core additions: read-only artefact sharing (directional and static, not collaborative), lightweight buyer translation layer (decision summaries, before/after comparisons), team token management with minimal buyer visibility.

*Proof required before building:* Artefacts from Tier 1/2 are already being shared upward or sideways. Teams are asking, not being sold.

**Product Line 2: BMAD Training (Methodology Ecosystem)**

**Tier B1 — BMAD Calibration Application**

The bridge from circuit to methodology. For users who've completed multiple circuit decisions and want to apply calibrated decision-making across BMAD workflows, agents, and artefacts. This is where BMAD is explicitly taught — but only to people who already have the calibration skill.

Core additions: BMAD workflow decision mapping (where calibration applies in each workflow), agent configuration calibration (when agent output is sufficient), scope and sufficiency decision practice using real BMAD artefacts.

*Proof required before building:* Circuit users are organically asking "how does this apply to my development work?" or discovering BMAD independently. The breadcrumb ("BMAD works with these decision constraints") is generating inbound interest.

**Tier B2 — Organisational BMAD Adoption**

Adds verifiable sufficiency and organisational compliance for teams adopting BMAD as a development methodology. Core additions: binary sufficiency verification by trained human assessors, audit trail for organisational compliance, certification that verifies calibration not excellence.

*Proof required before building:* Organisational adoption is occurring. Employer demand for verification exists organically. The certification can be issued without distorting the calibration experience.

**Feature Re-entry Sequence (Proof-Gated):**

*Circuit product line:*
1. Decision history with comparative continuity (Tier 2 gate)
2. Read-only artefact sharing (Tier 3 gate)
3. Constraint-based async peer artefact checks — non-evaluative (post distortion audit)

*BMAD training product line:*
4. BMAD workflow decision mapping (Tier B1 gate)
5. Human verification for certification — sufficiency only (Tier B2 gate)
6. Optional paid coaching — questions only, never validates decisions (late-stage)

**Long-Term Position:** The decision circuit becomes a **professional practice environment** — the place where people with consequential decisions come to calibrate. BMAD Training becomes the **methodology ecosystem** for people who want to apply that calibration across AI-native development. The circuit is the gateway; BMAD is the destination. Product naming, brand architecture, and the relationship between the two product lines should be resolved before Tier B1 development begins.

### Failure Mode Analysis: Judgement Circuit

The following failure modes were identified through systematic analysis of each move in the 5-move judgement circuit. Each represents a way the product could break, be gamed, or produce false positives in real-world use. Mitigations are proposed for PRD and UX design.

#### Move 0: Orientation

**Orientation is ignored (Likelihood: High / Impact: Medium):** A single sentence on a screen is the most skippable element in any digital product. Learners click through without reading. *Mitigation:* Orientation must require a micro-action — selecting "I have a real decision" vs "I'm exploring," with the exploring path ending immediately.

**Wrong decision granularity (Likelihood: Medium-High / Impact: High):** Learners arrive with decisions too trivial or too large for a single session. The circuit works only on decisions of the right consequential scale. *Mitigation:* Consider a framing prompt: "Bring a decision that someone else will see the result of." This filters for consequence without filtering for complexity.

#### Move 1: Eligibility

**Disqualification is gamed (Likelihood: High / Impact: Medium):** "Name a decision that doesn't qualify" is satisfied dishonestly with trivial answers ("what to have for lunch"). The gate becomes performative. *Mitigation:* Tighten to domain-relevant disqualification: "Name a decision in your current work that does NOT deserve this level of structure, and say why."

**Kill switch rate too high for commercial model (Likelihood: Medium / Impact: High):** If eligibility correctly stops >25-30% of sessions, paying customers feel they got nothing. *Mitigation:* **Resolved by token model** — eligibility failures do not consume a token. The learner keeps their token and can bring a different decision. High eligibility termination rates become calibration practice, not commercial risk. Post-eligibility classification prompt provides value even on failed attempts.

**Eligibility feels like rejection (Likelihood: Medium / Impact: High):** For indie builders with structure whiplash, being stopped triggers the oscillation pattern they're trying to escape. *Mitigation:* Termination language must reframe as achievement: "This decision doesn't need BMAD. That's good judgement. You just demonstrated it."

#### Move 2: Declaration

**Language blocking feels hostile (Likelihood: High / Impact: High):** Actively blocking phrases is experientially jarring. First-time users may not understand why their input is rejected. *Mitigation:* Blocking must be accompanied by immediate, specific, non-judgemental redirection: "That's describing what something else decided. Rewrite this as what *you* are deciding, and why."

**Linguistic ownership without cognitive ownership (Likelihood: Medium-High / Impact: High):** Smart learners replace "AI recommends X" with "I decided X" without actual shift in reasoning. The gate passes but calibration hasn't occurred. *Mitigation:* Add a consequence prompt: "What happens to *you* if this decision is wrong?" This forces consequence-awareness alongside language-awareness. Partly addressed by Moves 3-4 which require reasoning that can't be faked as easily.

**No examples causes paralysis (Likelihood: Medium / Impact: Medium):** Some learners — particularly accountability holders used to templates — genuinely don't know what a first-person decision declaration looks like. *Mitigation:* Consider a structural scaffold showing form without content: "I am deciding [what] because [why], and I accept [consequence]." A skeleton, not an example.

#### Move 3: Sufficiency

**Learners freeze at "name what is enough" (Likelihood: High / Impact: Critical):** Most professionals have never been asked to explicitly name sufficiency. The question may produce paralysis in learners whose careers are built on "more is better." *Mitigation:* Reframe as a filter question: "What would you need to know to act — and what would NOT change your decision even if you knew it?" This distinguishes productive friction ("harder but clearer") from blocking friction ("I don't know what you want").

**Risk field produces defensive hedging (Likelihood: Medium-High / Impact: Medium):** Asking someone to name accepted risk triggers risk-aversion — either minimisation ("the risk is low") or excessive hedging ("seventeen possible failure modes"). *Mitigation:* Constrain to a single sentence: "Name the one thing most likely to go wrong if you act on this decision." Singular, concrete, bounded.

**Responses too shallow to be meaningful (Likelihood: Medium / Impact: Medium):** Fields completed with "Enough: the data I have. Excluded: everything else. Risk: it might be wrong" — technically complete but hollow. *Mitigation:* Minimum quality thresholds checking for specificity: does the exclusion name a concrete thing, not a category? Does the risk name a consequence, not a probability?

#### Move 4: Artefact Assembly

**Artefact feels too small for price paid (Likelihood: High / Impact: High):** A half-page artefact from a £100+ programme feels disproportionately small compared to course competitors. *Mitigation:* Present as a professional decision record, not a training output. Visual design must convey weight — clean, formatted, export-ready. Value is in defensibility, not length. Positioning must establish this before purchase.

**Defence prompt has no enforcement (Likelihood: Medium-High / Impact: Medium):** "Could you defend this in under two minutes?" is a hypothetical with no consequence. Learners click "yes" reflexively. *Mitigation:* Replace with a structured challenge generated from their inputs: "Your rejected alternative was X. Why isn't X actually better?" At minimum, the prompt should be a write-in, not a yes/no.

**Data liability from real decisions (Likelihood: Low / Impact: Critical):** Saved artefacts contain real business decisions, potentially sensitive. This is not hypothetical training data. *Mitigation:* Encryption at rest, clear data retention policies, learner-controlled deletion. Non-negotiable security requirement for PRD.

#### Move 5: Stop/Go

**Commitment feels meaningless in training context (Likelihood: High / Impact: High):** Nothing actually happens when the learner commits — no code ships, no client sees it. The irreversibility is symbolic. *Mitigation:* Make irreversibility experiential: decision marked complete and cannot be edited without explicit reconsideration (separate action with its own friction). Artefact timestamped. If reconsidered, original version preserved alongside revision — making change of mind visible.

**"Not proceed" becomes comfortable default (Likelihood: Medium / Impact: Medium):** If a high percentage choose not to commit, the programme never produces its core irreversibility signal. *Mitigation:* Track commit/not-proceed ratio as internal diagnostic. If "not proceed" exceeds ~30%, investigate preceding moves. The "not proceed" path should also produce a saved artefact: "Decision deferred. Reason: [learner input]."

**Exit silence feels broken, not intentional (Likelihood: Medium / Impact: Medium):** For learners accustomed to digital products with next steps, silence may feel like a bug. *Mitigation:* Design the silence explicitly: "There is nothing else. This decision is yours now." Designed silence feels deliberate; absent next-steps feels incomplete.

#### Failure Mode Summary

| Move | Failure Mode | Likelihood | Impact | Priority for PRD |
|------|-------------|-----------|--------|-----------------|
| 0 | Orientation ignored | High | Medium | UX design |
| 0 | Wrong decision granularity | Medium-High | High | Onboarding |
| 1 | Disqualification gamed | High | Medium | Constraint tightening |
| 1 | Kill switch rate too high | Medium | High | Pricing/positioning |
| 1 | Eligibility feels like rejection | Medium | High | UX copy |
| 2 | Language blocking feels hostile | High | High | UX design (critical) |
| 2 | Linguistic ownership without cognitive | Medium-High | High | Add consequence prompt |
| 2 | No examples causes paralysis | Medium | Medium | Structural scaffold |
| 3 | Learners freeze at sufficiency | High | Critical | Reframe as filter (critical) |
| 3 | Risk field produces hedging | Medium-High | Medium | Single-sentence constraint |
| 3 | Responses too shallow | Medium | Medium | Specificity checks |
| 4 | Artefact feels too small | High | High | Positioning + formatting |
| 4 | Defence prompt unenforced | Medium-High | Medium | Structured challenge |
| 4 | Data liability | Low | Critical | Security requirements |
| 5 | Commitment feels meaningless | High | High | Experiential irreversibility |
| 5 | "Not proceed" as default | Medium | Medium | Diagnostic tracking |
| 5 | Silence feels broken | Medium | Medium | Designed exit state |

### What If Scenarios (Scope Pressure Testing)

The following scenarios test the MVP scope boundaries under realistic commercial and operational pressure.

#### Scenario 1: "40% of Sessions End at Eligibility"

**Trigger:** Eligibility kill switch works as designed, but the termination rate is commercially unsustainable. Nearly half of paying users never reach the core learning event.

**What breaks:** Revenue model. If the programme costs £100+ and 40% of sessions end before Move 2, refund pressure mounts. Buyers feel cheated. The "stop is success" framing becomes a liability rather than a differentiator.

**Key decision required:** Is the eligibility gate a pre-purchase filter or a post-purchase experience?

**Options to preserve integrity:**
- Move eligibility to a free pre-qualification step (before payment) — but this risks turning it into marketing
- Reframe pricing as "access to a decision environment" rather than "completion of a programme" — but this requires sophisticated positioning
- Accept the termination rate and build it into unit economics — requires higher price point or volume

**Implication for PRD:** **Resolved by token model.** Eligibility is post-purchase but failures do not consume a token. The learner can attempt eligibility unlimited times with different decisions. The paywall question is resolved: you pay for access to the circuit, not for eligibility. The eligibility step is now free calibration practice, making high termination rates a feature rather than a commercial liability.

#### Scenario 2: "The First 50 Learners All Ask 'What's Next?'"

**Trigger:** The Day-1 product test — "If they ask 'what should I do next?' the design has failed" — fires consistently. Every learner completes the circuit and immediately asks for continuation.

**What breaks:** The core design thesis. Either the silence-after-completion is genuinely experienced as incompleteness (not finality), or learners have been conditioned so deeply by digital products that designed silence cannot override expectation within a single session.

**Key decision required:** Is "what's next?" always a failure signal, or can it sometimes indicate engagement rather than incompleteness?

**Options to preserve integrity:**
- Distinguish between "what's next in this programme?" (failure) and "how do I use this on my next decision?" (success) — requires qualitative analysis of the question
- Add a single post-completion prompt: "Use this on a real decision this week. There is nothing else to learn here." — but this is dangerously close to a "next step"
- Accept that the first cohort may need iterative exit-state redesign

**Implication for PRD:** The exit state requires A/B testing capability from launch. The product brief should specify that the exit state copy is a tunable parameter, not a fixed string.

#### Scenario 3: "An Enterprise Buyer Requires Completion Reporting"

**Trigger:** A buyer with a £50k budget and 200 seats will not proceed without completion dashboards. The refused metrics policy (no progress indicators, no KPIs) directly blocks revenue.

**What breaks:** The metric governance policy. The buyer's requirement is reasonable from a procurement perspective — they need to justify spend. But providing completion reporting introduces exactly the performance optics the MVP refuses.

**Key decision required:** Is there a form of reporting that satisfies procurement without distorting learner behaviour?

**Options to preserve integrity:**
- Offer aggregate, anonymised completion data to buyers only (never visible to learners) — preserves diagnostic vs evaluative distinction
- Provide "participation confirmation" (binary: used/not used) without progress or performance detail
- Decline the deal and document it as validation that individual-first pricing is correct at MVP

**Implication for PRD:** The product brief should specify a "buyer reporting" capability that is architecturally separated from the learner experience. What buyers see and what learners see must be different systems — not different views of the same data.

#### Scenario 4: "3-Month Kill Criteria Trigger but Learner Feedback Is Glowing"

**Trigger:** At 3 months, the viability conditions are not met (e.g., fewer than 50 completions, no unprompted reuse signal), but qualitative feedback is overwhelmingly positive. Learners love it. They just don't return unprompted.

**What breaks:** The kill criteria's diagnostic precision. Positive feedback without behavioural change is precisely the "reassurance trap" the product is designed to resist — but killing a product that users praise requires extraordinary discipline.

**Key decision required:** Are the kill criteria measuring the right thing, or are they too narrow for a genuinely novel product category?

**Options to preserve integrity:**
- Extend the observation window (6 months instead of 3) if qualitative signal is strong but behavioural signal is absent — but this must be decided *before* launch, not at the 3-month mark
- Add a "delayed reuse" tracker that monitors whether reuse appears at 6-8 weeks rather than within 30 days
- Hold the kill criteria as written and accept that positive feedback without behaviour change is a failed product — the hardest but most intellectually honest option

**Implication for PRD:** The product brief should specify that kill criteria observation windows are decided pre-launch and are not adjustable under pressure. The PRD must include a "criteria lock" mechanism — a governance commitment that prevents moving the goalposts when results are uncomfortable.

#### Scenario 5: "A Competitor Launches 'AI Decision Training' at £29/month"

**Trigger:** A well-funded competitor launches a subscription product that uses AI to "train decision-making skills" with progress tracking, community features, gamification, and certification — everything the BMAD MVP deliberately refuses. It gains rapid traction and media coverage.

**What breaks:** Market positioning. The competitor occupies the "AI decision training" category with familiar, comfortable features. BMAD's sparse, constrained, silence-based approach looks primitive by comparison. Buyers ask: "Why would I pay more for less?"

**Key decision required:** Does the competitor validate the category (good) or define it in a way that makes BMAD's approach unintelligible to buyers (dangerous)?

**Options to preserve integrity:**
- Lean into the contrast: "They train you to feel confident. We train you to stop." — requires very strong positioning copy
- Accelerate Tier 2/3 development to demonstrate depth beyond single-session use
- Refuse to compete on feature count and instead compete on outcome evidence — but this requires proof data the MVP may not yet have

**Implication for PRD:** The product brief should specify that competitive positioning is outcome-based, never feature-based. Marketing and product language must be designed to make the "less is more" argument before a competitor forces the comparison.

#### What If Scenarios Summary

| Scenario | Core Tension | PRD Requirement |
|----------|-------------|-----------------|
| 40% eligibility termination | Commercial viability vs design integrity | Eligibility placement relative to paywall |
| Universal "what's next?" | Exit state design vs user conditioning | Tunable exit state with A/B capability |
| Enterprise reporting demand | Metric governance vs revenue | Separated buyer/learner reporting systems |
| Kill criteria vs positive feedback | Diagnostic precision vs qualitative signal | Pre-launch criteria lock mechanism |
| Competitor at £29/month | Premium positioning vs market definition | Outcome-based competitive positioning |
