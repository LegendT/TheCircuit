# Cross-Research Viability Assessment: 5-Move Judgement Circuit

**Date:** 2026-02-10
**Author:** Tone (with assessment facilitation by Mary, BMM Business Analyst)
**Assessment Type:** Cross-Research Synthesis — Go/No-Go Verdict
**Input Documents:**
- Market Research: Professional Decision Tools Market (2026-02-10)
- Domain Research: Decision Science and Constrained Professional Environments (2026-02-10)
- Technical Research: 5-Move Judgement Circuit Architecture and Implementation (2026-02-10)
- Product Brief: BMAD-TRAINING (2026-02-09)

---

## Verdict: GO

**Combined Viability Score: 72%**

The 5-Move Judgement Circuit is viable for MVP development. Three independent research streams confirm that the market gap is real, the science validates the design, and the technology exists to build it within budget and timeline. The product faces one existential risk (category failure) that cannot be mitigated through engineering — only through market execution.

This is a conditional go. The conditions are documented below as kill criteria.

---

## Scoring Methodology

Each research stream is scored across five dimensions (0–100). The combined score is a weighted average reflecting the relative importance of each dimension to MVP success.

| Dimension | Weight | Market | Domain | Technical | Weighted Score |
|-----------|--------|--------|--------|-----------|----------------|
| **Problem Validation** | 25% | 85 | 90 | N/A | 87 |
| **Solution Validation** | 25% | 60 | 85 | 80 | 73 |
| **Feasibility** | 20% | 55 | 75 | 90 | 72 |
| **Competitive Position** | 15% | 70 | 80 | 85 | 77 |
| **Risk Profile** | 15% | 45 | 65 | 80 | 61 |
| **Combined** | **100%** | **65** | **80** | **85** | **72** |

**Score interpretation:**
- 80–100: Strong go — proceed with high confidence
- 60–79: Conditional go — proceed with documented conditions and kill criteria
- 40–59: Cautious hold — material gaps require resolution before proceeding
- 0–39: No-go — fundamental viability question unanswered

At 72%, the circuit sits firmly in the conditional-go range. The score is pulled down by market uncertainty (the buyer has never been asked to buy this category of product) and pulled up by domain certainty (the science is unambiguous) and technical confidence (no blocking risks, well-understood stack).

---

## Stream 1: Market Viability — 65%

### What the Market Research Confirms

**The gap is real.** The professional decision tools market operates in three tiers: Enterprise ($10k–$500k+/year, ~$16.3B), Professional Instruments ($500–$5,000), and Consumer Apps (Free–$50). Between these — in the $50–$500 range serving individual professionals making consequential decisions — there is no product. This is not a thin slice of an existing market; it is an entirely unserved segment.

**The pain points are documented and intensifying:**
- Decision fatigue is universal (35,000 decisions/day) and unaddressed by existing tools
- AI is eroding professional judgement (18% confidence drop while adoption rose 13%)
- The "attributability gap" is widening — nobody can answer "who decided this?"
- Analysis paralysis is endemic at organisational scale
- The trust-transparency paradox means AI usage reduces stakeholder confidence

**Willingness-to-pay exists in adjacent markets:**
- Executive coaching validates $150–$3,000/hour for decision support
- Decision intelligence market growing at 15.4% CAGR
- Personal development market: $50.42B (2024), growing to $86.54B by 2034

**The competitive position is unique.** No direct competitor occupies the same position (mid-price, action-oriented, individual professional, artefact-producing). Five differentiators are unreplicated: constraint as product, professional artefact as output, token model, sufficiency boundary, calibration capture.

### What the Market Research Cannot Confirm

**The gap buyer exists but is unvalidated.** The "Decision Owner" — an individual professional spending £20–£50 on a constrained decision tool — is a constructed profile extrapolated from adjacent categories. No primary research (surveys, interviews, purchase data) validates that this person will actually pay. The profile is persuasive but theoretical.

**Category creation is the hardest go-to-market.** Blue ocean entries fail roughly 25–30% of the time. The circuit competes with inaction, not with products. If the buyer never recognises "constrained decision tools" as a category, the product has no market regardless of how good it is.

**Token model revenue is unpredictable.** Event-based purchasing (pay when you have a consequential decision) creates inherently volatile revenue. There is no SaaS precedent for per-decision pricing at this scale.

### Market Viability Verdict

The market opportunity is real but unproven. The gap exists. The pain points are documented. Willingness-to-pay exists in adjacent categories. But the critical question — "will the gap buyer recognise and purchase a constrained decision tool?" — can only be answered through market testing, not research.

**Market confidence: Medium (65%)**

---

## Stream 2: Domain Viability — 80%

### What the Domain Research Confirms

**The science is unambiguous.** Eight independent decision science theories — spanning 7 decades and 3 Nobel Prize winners (Simon, Kahneman, Thaler) — converge on a single finding: constraint, structure, and forced commitment improve decision quality. This is not contested in the literature.

| Theory | Key Finding | Circuit Feature Validated |
|--------|------------|--------------------------|
| Bounded Rationality (Simon) | Satisficing outperforms optimising | Sufficiency boundary |
| Less-Is-More (Gigerenzer) | Fewer cues → more accurate decisions | 5-move constraint |
| Paradox of Choice (Schwartz) | Fewer options → better decisions | Structured sequence |
| Dual Process (Kahneman) | System 2 requires environmental triggers | Each move as activation trigger |
| Decision Noise (Kahneman et al.) | Structured processes reduce total error | Circuit as decision hygiene protocol |
| Choice Architecture (Thaler/Sunstein) | Environment design shapes decision quality | Circuit as designed environment |
| Commitment Devices | Irreversibility improves satisfaction | Token model + progressive commitment |
| Cognitive Load (Sweller) | Reducing load improves outcomes | Sequential single-focus moves |

**Professional precedent is proven at the highest stakes:**
- Medicine: 19-item checklist cut surgical deaths by 47% (Gawande/WHO)
- Aviation: Boeing Model 299 checklist → 1.8 million miles without accident; FOR-DEC, CRM
- Military: OODA loop, Auftragstaktik (mission command), After Action Review
- Law: Sentencing guidelines reduced judicial inconsistency whilst preserving discretion

**The three distinctive mechanisms are individually validated:**
- Calibration Capture: grounded in Dunning-Kruger mitigation and Tetlock's superforecaster methodology
- Sufficiency Boundary: implements optimal stopping theory (37% rule)
- Artefact: leverages cognitive offloading and the Architecture Decision Record model

**The integrated feedback loop is novel.** Each mechanism is proven independently, but the specific combination (Calibration Capture → Sufficiency Boundary → Artefact → improved future calibration) has not been tested as an integrated system. The theoretical basis is strong — each mechanism feeds the others — but the integration is unproven.

### What the Domain Research Cannot Confirm

**Transfer from high-stakes to everyday decisions is assumed, not proven.** The four professional precedents operate in environments with institutional authority (hospitals, cockpits, courts, military command). The circuit applies these principles to voluntary, self-selected professional decisions without institutional backing. The transfer is well-supported theoretically but empirically untested.

**Constraint resistance is real.** Gawande documented physician resistance to surgical checklists. Professionals may reject constraint as demeaning, even when the evidence supports it. The circuit must position constraint as professional practice, not limitation.

### Domain Viability Verdict

The domain science provides the circuit's strongest strategic asset. Every core feature maps to multiple independent sources of validation. The scientific foundation is not marketing — it is the product's architecture. The only uncertainty is whether the transfer from institutional high-stakes environments to voluntary individual environments holds. Theoretical support is strong; empirical confirmation requires the product to exist.

**Domain confidence: High (80%)**

---

## Stream 3: Technical Viability — 85%

### What the Technical Research Confirms

**No blocking technical risks.** The circuit can be built with mainstream, well-documented technologies. Every architectural decision maps to established patterns with production-grade reference implementations.

**The stack is validated:**
- Next.js 16 (App Router, React Server Components) — largest ecosystem, most starter kit support
- Supabase (PostgreSQL, Auth, RLS) — unified backend with EU data residency
- Stripe (Checkout Sessions, webhooks) — credit/token system is a documented pattern
- Vercel + Cloudflare — standard deployment for Next.js SaaS
- React-PDF + Markdown — dual-format artefact export

**Starter kits eliminate weeks of boilerplate.** MakerKit, Supastarter, and the Vercel SaaS Starter provide pre-built auth, payment integration, and account management. Development effort focuses entirely on the circuit experience.

**The architecture is deliberately simple:**
- Modular monolith, not microservices — one codebase, one database, one deployment
- Linear state machine (`useReducer`) — no complex branching or parallel states
- Six integration boundaries — all well-documented, no exotic patterns
- Row Level Security — database-level data isolation from day one

**Infrastructure cost is remarkably low:**
- £45–65/month total (Vercel Pro + Supabase Pro + Cloudflare free + Resend free)
- Breakeven at 3 token sales per month (at £20/token) or 1 sale per month (at £50/token)
- No per-user infrastructure costs until significant scale

**Timeline is realistic:** 6–8 weeks for MVP with a starter kit, for a developer experienced in the stack. 8–11 weeks for a developer learning the stack.

**GDPR compliance is architecturally baked in:**
- CASCADE deletes remove all user data in one operation
- EU data residency (Frankfurt) for Supabase
- RLS enforces data isolation at the database level

**Multi-product forward planning is addressed:**
- Credit ledger supports future product parameter
- Single Supabase project principle preserves shared identity and calibration data

### What the Technical Research Cannot Confirm

**Artefact quality is a design problem, not a technical one.** React-PDF can generate professional documents. Whether the circuit's output *looks and feels* professional enough to share with colleagues is a design and content question, not a technology question. This is the product's primary conviction mechanism — if the artefact looks like a quiz result, the product fails.

**Starter kit fit is assumed.** The recommendation is to build on MakerKit or the Vercel SaaS Starter, but the circuit's constraint-based workflow is unusual. If the starter kit's assumptions conflict with the circuit's patterns (e.g., subscription-oriented billing vs token-oriented billing), refactoring may be required.

### Technical Viability Verdict

Technical execution is the lowest-risk dimension. The stack is mainstream, the patterns are documented, the costs are minimal, and the timeline is realistic. There are no novel technical challenges — every component has been built before in other products. The risk is not "can we build it?" but "will what we build convince the buyer?"

**Technical confidence: High (85%)**

---

## Cross-Research Synthesis: Where the Streams Converge and Diverge

### Convergence Points (Strengths)

**1. The problem is real across all three lenses.**
- Market: Six documented pain points with academic and industry citations
- Domain: Eight decision science theories explaining why these pain points exist
- Technical: The pain points map to solvable technical requirements

**2. The design is scientifically grounded and technically buildable.**
- Domain validates every circuit feature against multiple theories
- Technical confirms every feature can be implemented with mainstream technology
- The gap between "what the science says" and "what we can build" is zero

**3. The economic model is viable at small scale.**
- Market: Token pricing at £20–£50 is 10–40x cheaper than coaching, the nearest alternative
- Technical: Infrastructure costs of £45–65/month break even at 1–3 sales/month
- Domain: The token model functions as a commitment device (validated by behavioural science)

**4. The competitive position is genuinely unique.**
- Market: No product occupies the mid-price, action-oriented, individual professional position
- Domain: No product applies decision science's convergent evidence to constrained professional environments
- Technical: The 5-move state machine + calibration capture + artefact export is a novel combination but uses standard components

### Divergence Points (Tensions)

**1. Market uncertainty vs domain certainty.**
The domain research says "constraint works" with Very High confidence. The market research says "will the buyer pay for constraint?" with Medium confidence. The science cannot guarantee a market. This is the product's fundamental tension: the most validated design feature (constraint) may be the hardest to sell.

**2. Artefact centrality vs artefact uncertainty.**
All three streams identify the artefact as the product's most important output:
- Market: "artefact as marketing" — the growth engine
- Domain: cognitive offloading mechanism, ADR model, permanent decision record
- Technical: React-PDF export, shareable URLs, dual-format output
But no stream can confirm that the artefact will be *good enough*. Artefact quality is a design taste problem that sits outside research.

**3. Solo developer speed vs product quality.**
- Technical: 6–8 weeks is realistic for a developer experienced in the stack
- Market: The buyer evaluates on "immediate credibility" and "output value" — surface quality matters enormously
- Domain: The cold start and constraint design must feel professional, not experimental
A solo developer racing to ship in 8 weeks may sacrifice the polish that the market and domain analyses identify as critical.

---

## Critical Assumptions (Ranked by Impact)

These are the assumptions that the viability assessment depends on. If any prove false, the viability score changes materially.

### Assumption 1: The Gap Buyer Exists and Will Pay (Impact: Critical)

**The claim:** Individual professionals will pay £20–£50 per consequential decision for a constrained decision environment.

**Evidence for:** Adjacent market willingness-to-pay (coaching at £150–£3,000/hour), documented pain points (decision fatigue, AI judgement erosion, ownership gap), personal development market growing at 11%+ CAGR.

**Evidence against:** No product has tested this specific price point for this specific buyer. The gap buyer profile is constructed, not observed. Zero transaction data exists.

**How it fails:** The buyer exists but won't pay (prefers free AI assistants), or the buyer doesn't recognise the category (constraint feels like limitation, not value).

**Validation method:** Pre-launch email list engagement (2,000+ subscribers indicates problem recognition), soft launch conversion rate (>5% of trial users purchasing a token).

### Assumption 2: The Artefact Is Convincing Enough to Share (Impact: High)

**The claim:** The decision record produced by the circuit will be professional enough that users voluntarily share it with colleagues, creating organic growth.

**Evidence for:** ADR model validates the format. Cognitive offloading research confirms writing improves decisions. The artefact structure (orientation, eligibility, declaration, sufficiency, summary) is logically sound.

**Evidence against:** No artefact has been designed or tested. Professional credibility is a design taste judgment, not a research finding. If the output resembles a quiz result or a worksheet, the growth engine stalls.

**How it fails:** Users complete the circuit and find the output useful privately but not credible enough to share professionally. Share rate below 10% kills the organic growth assumption.

**Validation method:** Soft launch artefact share rate. Target: >20% of completed circuits shared publicly.

### Assumption 3: The Cold Start Works (Impact: High)

**The claim:** Dropping users into a constrained 5-move sequence without preamble, guidance, or tutorial is the correct design — not a UX oversight.

**Evidence for:** Domain research is unambiguous — every professional precedent (checklists, FOR-DEC, OODA) begins with action, not briefing. Cognitive Load Theory supports minimal preamble. The circuit's constraint IS the intervention.

**Evidence against:** Consumer and professional software users expect onboarding. A completely cold start may confuse users who have never encountered a constrained decision environment. Dropout before Move 2 would indicate confusion, not insufficient commitment.

**How it fails:** Users arrive, see no guidance, and abandon before understanding what the circuit does. Completion rate below 50% of started circuits suggests the cold start is too cold.

**Validation method:** Soft launch completion rate by move. If >30% drop between Move 1 and Move 2, the cold start needs refinement (not elimination — refinement).

### Assumption 4: Category Creation Is Achievable with Content Marketing (Impact: High)

**The claim:** Content marketing that "names the pain before naming the solution" (decision fatigue, AI judgement erosion, analysis paralysis) will create category awareness without enterprise marketing budgets.

**Evidence for:** Community-led growth is the dominant GTM for professional tools in 2026. The professional precedent narrative (surgeons use checklists, pilots use FOR-DEC) is inherently shareable. Content marketing CAC is lower than paid acquisition.

**Evidence against:** Category creation typically requires sustained investment over 12–18 months. A solo founder has limited bandwidth for both building and marketing. The market research identifies a 25% probability of category failure.

**How it fails:** Content generates intellectual interest but not purchase intent. People find the concept interesting but don't have a pending decision worth £20–£50 at the moment of discovery.

**Validation method:** Pre-launch email list size (target: 2,000+), content engagement metrics, and conversion from email list to soft launch purchase.

### Assumption 5: The Token Model Creates Sufficient Revenue Predictability (Impact: Medium)

**The claim:** Event-based purchasing (buy tokens when you have a consequential decision) generates enough revenue to sustain operations and growth.

**Evidence for:** Per-use consumption pricing is growing (56% of software providers expect usage-based revenue growth by 2027). The breakeven point is extremely low (1–3 sales/month). The token model eliminates subscription anxiety.

**Evidence against:** SaaS investors and lenders value recurring revenue. Event-based models are inherently volatile. No comparable product uses per-decision pricing at this scale.

**How it fails:** Users purchase one token, complete one circuit, and don't return for months. Monthly revenue oscillates wildly, making investment and hiring impossible without subscription revenue as a baseline.

**Validation method:** Repeat purchase rate. Target: >20% of users purchase a second token within 6 months.

---

## Kill Criteria

These are the specific, measurable conditions under which the project should be stopped or fundamentally pivoted. They are defined now so that future decisions are not influenced by sunk cost.

### Kill Criterion 1: Pre-Launch Demand Signal Failure

**Trigger:** Fewer than 500 email subscribers after 3 months of content marketing.

**What it means:** The problem framing ("decision fatigue," "AI judgement erosion," "analysis paralysis") does not resonate enough to generate interest. If people don't sign up to learn about the problem, they won't pay to solve it.

**Action:** Stop MVP development. Pivot the problem framing or target audience before resuming.

### Kill Criterion 2: Soft Launch Conversion Failure

**Trigger:** Fewer than 5% of soft launch invitees (target: 100–500 people) purchase a token within 30 days.

**What it means:** The gap buyer recognises the problem but does not see the circuit as the solution, or the price point is wrong, or the artefact preview is unconvincing.

**Action:** Pause marketing spend. Investigate through user interviews: is the problem wrong, the solution wrong, or the price wrong? Iterate before public launch.

### Kill Criterion 3: Completion Rate Collapse

**Trigger:** Fewer than 50% of started circuits reach the artefact stage.

**What it means:** The circuit's constraint design is too confusing, too demanding, or perceived as too restrictive. Users are paying but not finishing.

**Action:** Investigate dropout by move. If dropout concentrates at Move 1→2 (cold start confusion), add minimal orientation. If dropout is distributed (general fatigue), the circuit is too demanding for the buyer.

### Kill Criterion 4: Artefact Worthlessness

**Trigger:** Fewer than 10% of completed circuits result in a shared artefact after 3 months.

**What it means:** Users complete the circuit and find private value, but the output is not professional enough to share. The organic growth engine is dead.

**Action:** Redesign the artefact format. Commission professional design review. Test revised artefact with a new cohort. If sharing still doesn't happen, the artefact-as-marketing strategy needs replacement (paid acquisition, partnerships, etc.).

### Kill Criterion 5: Zero Repeat Purchases

**Trigger:** Fewer than 10% of users purchase a second token within 6 months.

**What it means:** The circuit provides one-time value but not ongoing professional utility. The product is a novelty, not a practice.

**Action:** Investigate through calibration capture data and user interviews. Does the user face another consequential decision? Did the first circuit feel valuable enough to repeat? If not, consider subscription model with unlimited circuits, or pivot to team/organisational offering.

---

## Viability Matrix: All Dimensions

| Dimension | Score | Rationale | Key Risk |
|-----------|-------|-----------|----------|
| Problem Validation | 87% | Six documented pain points, academic and industry evidence, intensifying with AI adoption | Pain is real but diffuse — the buyer may not recognise it as a solvable problem |
| Solution Validation | 73% | Science validates constraint, precedent validates structure, but specific implementation untested | Artefact quality and cold start UX are design taste problems, not research problems |
| Technical Feasibility | 90% | Mainstream stack, documented patterns, low cost, realistic timeline | Solo developer polish vs buyer expectations |
| Market Feasibility | 55% | Gap exists, buyer profile constructed, willingness-to-pay inferred from adjacent markets | Category creation is the existential risk — if the buyer never recognises the category, nothing else matters |
| Competitive Position | 77% | No direct competitor, five unreplicated differentiators, constraint design as moat | ChatGPT/Notion could replicate features (30% probability); constraint philosophy harder to copy |
| Financial Viability | 78% | £45–65/month costs, breakeven at 1–3 sales/month, token model validated in adjacent categories | Revenue unpredictability from event-based model |
| Risk Profile | 61% | Existential risk (category failure 25%), operational risks well-mitigated, no technical blocking risks | Category failure cannot be mitigated through engineering — only through market execution |
| **Overall** | **72%** | **Conditional go — proceed with kill criteria and validation milestones** | **Category creation is the make-or-break factor** |

---

## Conditions for Proceeding

The go verdict is conditional on the following:

1. **Content marketing begins before or parallel to MVP development** — the circuit's greatest risk (category failure) is mitigated by demand validation. Building without marketing generates a product nobody knows they need.

2. **Artefact design receives disproportionate attention** — the artefact is simultaneously the product's output, its marketing vehicle, and its credibility proof. Rushing the artefact design to meet a timeline undermines all three.

3. **Soft launch precedes public launch** — 100–500 invite-only users provide the conversion, completion, and sharing data needed to validate or invalidate the five critical assumptions. Public launch without this data is gambling.

4. **Kill criteria are honoured** — the criteria above are defined now specifically so they cannot be rationalised away later. If a kill criterion is triggered, the project pauses for investigation, not for coping.

5. **The cold start and constraint design are protected** — domain research is unambiguous: weakening the constraint for user comfort designs against the science. If completion rates are low, investigate confusion (fixable) before assuming the constraint is wrong (it isn't).

---

## Strategic Recommendation

**Build the MVP. Start content marketing immediately. Soft launch within 8–10 weeks. Measure against kill criteria. Decide on public launch based on data.**

The circuit occupies a position that is scientifically validated, technically buildable, economically viable at micro-scale, and competitively unique. The only question that research cannot answer — "will the gap buyer pay?" — can only be answered by building the product and putting it in front of people.

The risk of building is low (£45–65/month infrastructure, 6–8 weeks of development time, recoverable investment). The risk of not building is opportunity cost — the market conditions (AI judgement erosion, decision fatigue awareness, ADR adoption) are favourable *now* and may not remain so indefinitely.

**The next step is architecture.** The technical research provides a complete technology stack recommendation. An architecture document should translate this into specific implementation decisions — database schema finalisation, API contract design, state machine specification, and artefact format definition.

---

**Assessment Completion Date:** 2026-02-10
**Combined Viability Score:** 72% (Conditional Go)
**Recommendation:** Proceed to architecture phase with parallel content marketing
**Kill Criteria:** 5 defined, measurable, time-bound
**Critical Assumptions:** 5 documented with validation methods
