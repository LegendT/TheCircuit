# BMAD v6 Training – Measuring Learner Impact Without Distortion (Refined)

This document defines **how to know the BMAD v6 Training Programme is working for learners** without collapsing judgement training into metrics theatre.

It refines the earlier framing by:
- Tightening what is genuinely observable
- Removing signals that are too indirect or too easy to game
- Making measurement *diagnostic*, not evaluative

The aim is to operationalise the north-star outcome:

> **“I trust my own decisions again — and I can explain them.”**

---

## What Was Slightly Overstated

### 1. Artefact quality as a proxy for judgement

**Original risk**: Treating artefact improvements as a reliable stand-in for judgement quality.

**Refinement**:
- Artefacts can improve cosmetically without judgement improving
- Some learners produce clean artefacts while still deferring internally

**Design implication**:
- Artefacts are *necessary but insufficient* signals
- They must be paired with evidence of ownership

---

### 2. Defence speed as a standalone indicator

**Original risk**: Over-weighting speed of explanation.

**Refinement**:
- Fast answers can indicate:
  - Confidence
  - Or rehearsed rationalisation

**Design implication**:
- Defence speed matters **only when paired with omission awareness**
- Speed without conscious trade-offs is noise

---

### 3. Reduced anxiety inferred too easily

**Original risk**: Assuming less rework or hesitation always reflects trust.

**Refinement**:
- Reduced rework can also signal disengagement
- Silence is ambiguous

**Design implication**:
- Anxiety reduction must be inferred through *decision finality*, not absence of activity

---

## What Was Missing

### 1. Ownership as the central observable

**Missing emphasis**: The most reliable signal of calibration is **ownership**.

Ownership means:
- The learner uses first-person reasoning
- They accept consequences
- They do not outsource judgement to AI, frameworks, or authority

This must be explicitly checked.

---

### 2. Stability under light pressure

**Missing element**: Judgement only reveals itself when gently challenged.

BMAD does not need stress testing — it needs **light, structured questioning**.

---

### 3. Negative capability

**Missing element**: The ability to *not* act.

Calibration shows up as:
- Comfort with leaving things undone
- Explicit rejection of plausible alternatives

This was implied but not foregrounded.

---

## Refined Measurement Model

Judgement cannot be measured directly.

It can be **triangulated** across three observable dimensions:

1. **Ownership**
2. **Stability under questioning**
3. **Sufficiency boundaries**

---

## 1. Ownership Signals (Primary)

### What to observe

Look for language and structure that indicate the learner owns the decision.

**Positive signals**:
- “I chose…”
- “I rejected X because…”
- “This risk is acceptable because…”

**Negative signals**:
- “The framework suggests…”
- “AI recommended…”
- “Best practice says…”

Ownership must be explicit, not implied.

---

## 2. Stability Under Light Questioning

### How to test

Apply **one mild challenge**, such as:

> “Why not the obvious alternative?”

### What to observe

Before calibration:
- Defensive tone
- Excessive explanation
- Appeal to authority

After calibration:
- Short clarification
- Willingness to pause
- Clear restatement of trade-off

**Key signal**: the learner does not escalate.

---

## 3. Sufficiency Boundaries (Chosen Non-Work)

### What to observe

Look for:
- Explicit stop decisions
- Named exclusions
- Clear acceptance of trade-offs

Examples:
- “We are not doing X because it doesn’t change the decision.”
- “This is sufficient for the current risk profile.”

**Absence of boundaries = absence of calibration.**

---

## The Minimal Viable Measurement Set

To know the programme is working, require **only this**:

### Learner submits:

1. One real decision artefact
2. One short defence answering:
   > “Why this, not the most obvious alternative?”
3. One explicit omission or stop decision

### Check only:

- Is the decision owned?
- Is the defence stable under one question?
- Is the omission conscious and justified?

No scoring. No ranking.

Binary diagnostic: calibration present / not yet present.

---

## What Not to Measure (Reinforced)

These distort behaviour and must be avoided:

- Completion rates
- Time-on-platform
- Confidence self-ratings
- Peer comparison
- Gamified progress

They reward performance, not judgement.

---

## The Hard Diagnostic Question

At the end of the MVP, ask:

> “Before this, how would you have justified this decision? How do you justify it now?”

You are not looking for correctness.
You are looking for **a change in reasoning posture**.

No shift = design failure.

---

## The Meta-Signal (Refined)

When BMAD is working, learners stop asking:

> “Am I doing this right?”

And start asking:

> “Is this sufficient for the risk I’m carrying?”

That question is not taught.
It emerges.

---

## Final Design Constraint

Any measurement added must satisfy this test:

> *Does this make learners rely more on their own judgement — or on the metric itself?*

If it’s the latter, it does not belong.

---

*This document should guide evaluation design, certification gating, automated checks, and internal discussions about learner impact for the BMAD v6 Training Programme.*