# BMAD v6 Training – Metrics We Deliberately Refuse to Track (Refined)

This document is a **metric-governance guardrail** for the BMAD v6 Training Programme.

Its purpose is not minimalism.
It is to prevent **measurement from becoming an authority substitute** in a system designed to restore judgement.

This refinement sharpens distinctions, removes a few over-broad claims, and makes refusal criteria more operational.

---

## What Was Slightly Overstated

### 1. “All engagement is avoidance” was too absolute

**Original risk**: Implying engagement metrics are *always* meaningless.

**Refinement**:
- Engagement is not inherently bad
- It is simply **non-diagnostic** for judgement

**Corrected position**:
- Engagement may signal *interest* or *confusion*
- It does not signal calibration

**Design implication**: Engagement can be monitored privately for usability issues, but never elevated to success criteria.

---

### 2. Completion rates treated as uniformly harmful

**Original risk**: Overstating the damage caused by completion metrics in all contexts.

**Refinement**:
- Completion becomes harmful when:
  - It is rewarded
  - It is compared
  - It is reported upward

**Corrected position**:
- Silent completion monitoring can be useful for detecting structural friction
- Public or buyer-facing completion metrics are corrosive

---

### 3. Certification pass rates dismissed too early

**Original risk**: Collapsing all pass-rate tracking into “teaching to the test.”

**Refinement**:
- Pass rates become distorting only when optimised
- Binary sufficiency gates can coexist with integrity

**Design implication**: Certification may track *eligibility*, but never *performance distribution*.

---

## What Was Missing

### 1. A clear refusal test (decision rule)

**Missing element**: A crisp way to decide whether a new metric belongs.

**Refinement**:
A metric must be refused if **any** of the following are true:

- Learners could optimise for it without improving judgement
- It creates social comparison or ranking
- It shifts authority away from the learner
- It incentivises continuation over sufficiency

If unsure, default to refusal.

---

### 2. Distinction between diagnostic vs evaluative metrics

**Missing element**: Not all measurement serves the same function.

**Refinement**:
- **Diagnostic metrics**: used privately to improve the system
- **Evaluative metrics**: used publicly to judge success

BMAD may tolerate the former.
It must resist the latter.

---

### 3. Buyer pressure scenarios

**Missing element**: Why these metrics will be requested.

**Refinement**:
Metrics are usually demanded when:
- Spend increases
- Responsibility diffuses
- Risk is externalised

This is predictable.
Refusal must be principled, not reactive.

---

## Refined Refusal Categories

### Category A: Authority-Substituting Metrics (Hard No)

These metrics directly replace learner judgement.

- Peer ratings / upvotes
- Instructor scores
- “Expert” evaluations of thinking

**Rule**: Never introduce.

---

### Category B: Behaviour-Distorting Metrics (Hard No)

These change behaviour in ways that undermine calibration.

- Leaderboards
- Gamified progress
- Speed benchmarks
- Volume of artefacts

**Rule**: Never introduce.

---

### Category C: Buyer-Comfort Metrics (Contextual No)

These reassure stakeholders without improving outcomes.

- Learning hours
- Completion percentages
- Engagement dashboards

**Rule**:
- May exist as *internal diagnostics*
- Must never be framed as success

---

## Refined Positions on Common KPIs

### Completion Rate

- ❌ Not a success metric
- ⚠️ Allowed as silent friction detection
- ❌ Never reported upward

---

### Time Spent

- ❌ Never interpreted as value
- ⚠️ Allowed only to detect confusion or blockage

---

### Engagement Activity

- ❌ Never equated with learning
- ⚠️ Used only to identify usability problems

---

### Certification Pass Rate

- ⚠️ Binary sufficiency only
- ❌ No grading, ranking, or trend optimisation

---

## The Underlying Failure Mode These Metrics Create

When tracked improperly, these metrics:
- Teach learners to *perform learning*
- Teach organisations to *purchase reassurance*
- Teach programmes to optimise optics

All three are antithetical to BMAD’s purpose.

---

## What We Track by Refusing

By refusing these metrics, we protect space for:
- Ownership
- Boundary clarity
- Stability under questioning
- Reuse under pressure

These are harder to count.
That difficulty is the cost of integrity.

---

## Final Metric Admission Test (Refined)

Before admitting *any* metric, ask:

1. Would learners change behaviour if they knew this was measured?
2. Could someone improve this metric without improving judgement?
3. Would this ever be shown to a buyer or used for comparison?

If **yes** to any → refuse.

---

*This refined document should be treated as a standing policy for analytics, reporting, certification design, and buyer negotiations for the BMAD v6 Training Programme.*